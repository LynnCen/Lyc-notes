import{_ as a,c as l,o as i,a7 as r}from"./chunks/framework.BQb8NfN9.js";const u=JSON.parse('{"title":"逻辑回归","description":"","frontmatter":{},"headers":[],"relativePath":"ai/机器学习/逻辑回归.md","filePath":"ai/机器学习/逻辑回归.md","lastUpdated":1769237553000}'),o={name:"ai/机器学习/逻辑回归.md"};function e(n,t,s,h,d,g){return i(),l("div",null,t[0]||(t[0]=[r('<h1 id="逻辑回归" tabindex="-1">逻辑回归 <a class="header-anchor" href="#逻辑回归" aria-label="Permalink to &quot;逻辑回归&quot;">​</a></h1><h2 id="概述" tabindex="-1">概述 <a class="header-anchor" href="#概述" aria-label="Permalink to &quot;概述&quot;">​</a></h2><p>逻辑回归（Logistic Regression）是一种广泛应用于二分类问题的统计学习方法。尽管名字中包含&quot;回归&quot;，但它实际上是一种分类算法。</p><h3 id="基本特点" tabindex="-1">基本特点 <a class="header-anchor" href="#基本特点" aria-label="Permalink to &quot;基本特点&quot;">​</a></h3><ul><li><strong>本质</strong>：分类问题（通常用于二分类：0 或 1）</li><li><strong>模型输出</strong>：原始输出是连续值（−∞ ~ +∞）</li><li><strong>核心机制</strong>：使用Sigmoid函数将连续输出压缩到[0,1]区间</li><li><strong>概率解释</strong>：压缩后的结果解释为&quot;样本属于类别 1 的概率&quot;</li></ul><h3 id="核心思想" tabindex="-1">核心思想 <a class="header-anchor" href="#核心思想" aria-label="Permalink to &quot;核心思想&quot;">​</a></h3><p>逻辑回归的核心是：使用一个线性模型得到连续输出 z，再使用Sigmoid函数把 z 压缩到 0-1 之间。将结果解释为&quot;样本属于类别1的概率&quot;，最终用阈值（通常为0.5）实现分类。</p><h3 id="为什么叫-回归" tabindex="-1">为什么叫&quot;回归&quot;？ <a class="header-anchor" href="#为什么叫-回归" aria-label="Permalink to &quot;为什么叫&quot;回归&quot;？&quot;">​</a></h3><p>逻辑回归之所以被称为&quot;回归&quot;，是因为它使用了线性回归的思想来建立模型，只是通过Sigmoid函数将输出转换为概率值，从而用于分类任务。</p><h2 id="数学原理" tabindex="-1">数学原理 <a class="header-anchor" href="#数学原理" aria-label="Permalink to &quot;数学原理&quot;">​</a></h2><h3 id="sigmoid函数" tabindex="-1">Sigmoid函数 <a class="header-anchor" href="#sigmoid函数" aria-label="Permalink to &quot;Sigmoid函数&quot;">​</a></h3><p>Sigmoid函数是逻辑回归的核心，它将任意实数映射到(0,1)区间：</p><p>$$\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{1 + e^z}$$</p><p><strong>Sigmoid函数的特性：</strong></p><ul><li><strong>值域</strong>：(0, 1)，输出可以解释为概率</li><li><strong>单调性</strong>：严格单调递增</li><li><strong>对称性</strong>：关于点(0, 0.5)中心对称</li><li><strong>导数特性</strong>：$\\sigma&#39;(z) = \\sigma(z)(1 - \\sigma(z))$，这个特性使得梯度计算非常方便</li></ul><p><strong>Sigmoid函数图像特点：</strong></p><ul><li>当 z → +∞ 时，σ(z) → 1</li><li>当 z → -∞ 时，σ(z) → 0</li><li>当 z = 0 时，σ(z) = 0.5</li></ul><h3 id="逻辑回归模型" tabindex="-1">逻辑回归模型 <a class="header-anchor" href="#逻辑回归模型" aria-label="Permalink to &quot;逻辑回归模型&quot;">​</a></h3><p>对于二分类问题，逻辑回归模型的数学表达式为：</p><p>$$h_\\theta(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$$</p><p>其中：</p><ul><li>$h_\\theta(x)$ 表示样本 x 属于类别 1 的概率</li><li>$\\theta$ 是模型参数（权重向量）</li><li>$x$ 是特征向量</li><li>$\\theta^T x = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$ 是线性组合</li></ul><h3 id="决策规则" tabindex="-1">决策规则 <a class="header-anchor" href="#决策规则" aria-label="Permalink to &quot;决策规则&quot;">​</a></h3><p>给定阈值 t（通常为 0.5），分类决策规则为：</p><ul><li>如果 $h_\\theta(x) \\geq t$，预测为类别 1</li><li>如果 $h_\\theta(x) &lt; t$，预测为类别 0</li></ul><h2 id="决策边界" tabindex="-1">决策边界 <a class="header-anchor" href="#决策边界" aria-label="Permalink to &quot;决策边界&quot;">​</a></h2><p><strong>定义</strong>：决策边界是把分类问题中的&quot;0 类&quot;和&quot;1 类&quot;分隔开的那条线／面／曲面。</p><p><strong>数学表达</strong>：决策边界由方程 $\\theta^T x = 0$ 确定，即：</p><p>$$\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n = 0$$</p><p><strong>特点：</strong></p><ul><li>对于二维特征空间，决策边界是一条直线</li><li>对于三维特征空间，决策边界是一个平面</li><li>对于更高维空间，决策边界是一个超平面</li><li>决策边界是线性的，这是逻辑回归的一个重要特性</li></ul><h2 id="成本函数" tabindex="-1">成本函数 <a class="header-anchor" href="#成本函数" aria-label="Permalink to &quot;成本函数&quot;">​</a></h2><h3 id="为什么不能使用mse" tabindex="-1">为什么不能使用MSE？ <a class="header-anchor" href="#为什么不能使用mse" aria-label="Permalink to &quot;为什么不能使用MSE？&quot;">​</a></h3><p><strong>问题</strong>：为什么逻辑回归不能使用线性回归的 MSE（均方误差）作为成本函数？</p><p><strong>答案</strong>：使用MSE会出现两个严重问题：</p><ol><li><p><strong>梯度下降不稳定</strong>：MSE + Sigmoid函数的目标函数会变成非凸曲面，可能有多个局部最优，不像线性回归那样有唯一的最优解（一个凸函数）。这会导致梯度下降算法难以找到全局最优解。</p></li><li><p><strong>概率模型不合理</strong>：分类本质是判断标签属于某个类的概率，MSE不能很好表达概率差异。例如，真实标签为1，预测概率为0.9和0.1时，MSE的惩罚可能不够明显。</p></li></ol><h3 id="交叉熵损失函数" tabindex="-1">交叉熵损失函数 <a class="header-anchor" href="#交叉熵损失函数" aria-label="Permalink to &quot;交叉熵损失函数&quot;">​</a></h3><p>逻辑回归使用**交叉熵损失（Cross-Entropy Loss）**作为成本函数，也称为对数损失（Log Loss）。</p><p><strong>单个样本的损失函数：</strong></p><p>$$L(y, h_\\theta(x)) = -[y \\log(h_\\theta(x)) + (1-y) \\log(1 - h_\\theta(x))]$$</p><p>其中：</p><ul><li>$y$ 是真实标签（0 或 1）</li><li>$h_\\theta(x)$ 是预测概率</li></ul><p><strong>损失函数的特性：</strong></p><ul><li>当 $y = 1$ 时：$L = -\\log(h_\\theta(x))$，预测概率越接近1，损失越小</li><li>当 $y = 0$ 时：$L = -\\log(1 - h_\\theta(x))$，预测概率越接近0，损失越小</li><li>当预测完全错误时（$y=1$但$h_\\theta(x) \\to 0$），损失趋向于无穷大</li></ul><p><strong>所有样本的平均损失（成本函数）：</strong></p><p>$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))]$$</p><p>其中 $m$ 是训练样本数量。</p><p><strong>为什么交叉熵损失更好？</strong></p><ol><li><strong>凸函数特性</strong>：交叉熵损失是凸函数，保证梯度下降能找到全局最优解</li><li><strong>概率解释</strong>：与最大似然估计等价，符合概率模型的统计原理</li><li><strong>梯度特性</strong>：梯度计算简单且稳定，有利于优化算法收敛</li></ol><h2 id="拟合与泛化" tabindex="-1">拟合与泛化 <a class="header-anchor" href="#拟合与泛化" aria-label="Permalink to &quot;拟合与泛化&quot;">​</a></h2><p>拟合：</p><p>模型对训练数据的适应程度。在逻辑回归中，拟合是指模型通过学习训练数据，找到能够区分不同类别的决策边界的过程。一个好的拟合应该能够捕捉到数据中的真实模式。</p><p>欠拟合：</p><p>模型过于简单，无法捕捉到数据中的基本模式。在逻辑回归中，欠拟合可能表现为：</p><ul><li>特征不足或特征选择不当</li><li>模型复杂度太低（如线性决策边界无法处理非线性可分的数据）</li><li>训练不充分</li><li>表现：训练集和测试集上的准确率都很低</li></ul><p>过拟合：</p><p>模型过度学习了训练数据的细节和噪声，导致对训练数据表现很好，但对新数据表现很差。在逻辑回归中，过拟合可能表现为：</p><ul><li>模型过于复杂（如使用了过多特征或高阶特征）</li><li>训练数据量太少</li><li>决策边界过于复杂，完美地分割了训练数据但无法泛化</li><li>表现：训练集准确率很高，但测试集准确率明显下降</li></ul><p>泛化：</p><p>模型对未见过的数据的预测能力。泛化能力是衡量模型好坏的核心指标。在逻辑回归中：</p><ul><li>好的泛化意味着模型学到的决策边界能够适用于新的数据样本</li><li>泛化能力通过测试集或验证集的性能来评估</li><li>欠拟合和过拟合都会降低模型的泛化能力</li><li>理想状态是在训练集和测试集上都表现良好，说明模型既学到了数据中的真实模式，又不会过度记忆训练数据</li></ul><h2 id="梯度下降算法" tabindex="-1">梯度下降算法 <a class="header-anchor" href="#梯度下降算法" aria-label="Permalink to &quot;梯度下降算法&quot;">​</a></h2><h3 id="梯度计算" tabindex="-1">梯度计算 <a class="header-anchor" href="#梯度计算" aria-label="Permalink to &quot;梯度计算&quot;">​</a></h3><p>为了最小化成本函数，需要计算梯度。对于逻辑回归，梯度计算公式为：</p><p>$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$</p><p><strong>向量形式：</strong></p><p>$$\\nabla_\\theta J(\\theta) = \\frac{1}{m} X^T (h_\\theta(X) - y)$$</p><p>其中：</p><ul><li>$X$ 是特征矩阵（$m \\times n$）</li><li>$y$ 是标签向量（$m \\times 1$）</li><li>$h_\\theta(X)$ 是预测概率向量</li></ul><h3 id="梯度下降更新规则" tabindex="-1">梯度下降更新规则 <a class="header-anchor" href="#梯度下降更新规则" aria-label="Permalink to &quot;梯度下降更新规则&quot;">​</a></h3><p>$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$</p><p>其中 $\\alpha$ 是学习率（learning rate）。</p><p><strong>批量梯度下降（Batch Gradient Descent）：</strong></p><p>每次迭代使用所有训练样本计算梯度。</p><p><strong>随机梯度下降（Stochastic Gradient Descent）：</strong></p><p>每次迭代随机选择一个样本计算梯度，更新速度快但可能不稳定。</p><p><strong>小批量梯度下降（Mini-batch Gradient Descent）：</strong></p><p>每次迭代使用一小批样本（如32、64个）计算梯度，平衡了速度和稳定性。</p><h3 id="学习率选择" tabindex="-1">学习率选择 <a class="header-anchor" href="#学习率选择" aria-label="Permalink to &quot;学习率选择&quot;">​</a></h3><ul><li><strong>学习率过大</strong>：可能导致损失函数震荡，甚至发散</li><li><strong>学习率过小</strong>：收敛速度慢，训练时间长</li><li><strong>自适应学习率</strong>：可以使用Adam、RMSprop等优化算法自动调整学习率</li></ul><h2 id="正则化" tabindex="-1">正则化 <a class="header-anchor" href="#正则化" aria-label="Permalink to &quot;正则化&quot;">​</a></h2><p>正则化是防止过拟合的重要技术，通过在损失函数中添加惩罚项来限制模型复杂度。</p><h3 id="l2正则化-ridge正则化" tabindex="-1">L2正则化（Ridge正则化） <a class="header-anchor" href="#l2正则化-ridge正则化" aria-label="Permalink to &quot;L2正则化（Ridge正则化）&quot;">​</a></h3><p>在成本函数中添加L2范数惩罚项：</p><p>$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$$</p><p>其中 $\\lambda$ 是正则化参数。</p><p><strong>特点：</strong></p><ul><li>使参数值趋向于0，但不会完全为0</li><li>对所有参数进行平滑的惩罚</li><li>有助于处理特征之间的多重共线性问题</li></ul><h3 id="l1正则化-lasso正则化" tabindex="-1">L1正则化（Lasso正则化） <a class="header-anchor" href="#l1正则化-lasso正则化" aria-label="Permalink to &quot;L1正则化（Lasso正则化）&quot;">​</a></h3><p>在成本函数中添加L1范数惩罚项：</p><p>$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |\\theta_j|$$</p><p><strong>特点：</strong></p><ul><li>可以将某些参数压缩到0，实现特征选择</li><li>产生稀疏模型，有助于特征选择</li><li>对异常值更敏感</li></ul><h3 id="弹性网络-elastic-net" tabindex="-1">弹性网络（Elastic Net） <a class="header-anchor" href="#弹性网络-elastic-net" aria-label="Permalink to &quot;弹性网络（Elastic Net）&quot;">​</a></h3><p>结合L1和L2正则化：</p><p>$$J(\\theta) = \\text{原始损失} + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\lambda_2 \\sum_{j=1}^{n} \\theta_j^2$$</p><p><strong>正则化参数 $\\lambda$ 的选择：</strong></p><ul><li>$\\lambda$ 越大，正则化强度越大，模型越简单</li><li>通常通过交叉验证来选择最优的 $\\lambda$ 值</li></ul><h2 id="多分类逻辑回归" tabindex="-1">多分类逻辑回归 <a class="header-anchor" href="#多分类逻辑回归" aria-label="Permalink to &quot;多分类逻辑回归&quot;">​</a></h2><h3 id="one-vs-rest-ovr" tabindex="-1">One-vs-Rest（OvR） <a class="header-anchor" href="#one-vs-rest-ovr" aria-label="Permalink to &quot;One-vs-Rest（OvR）&quot;">​</a></h3><p>将多分类问题转化为多个二分类问题：</p><ul><li>对于K个类别，训练K个二分类器</li><li>第k个分类器将第k类作为正类，其他所有类作为负类</li><li>预测时，选择概率最高的类别</li></ul><h3 id="one-vs-one-ovo" tabindex="-1">One-vs-One（OvO） <a class="header-anchor" href="#one-vs-one-ovo" aria-label="Permalink to &quot;One-vs-One（OvO）&quot;">​</a></h3><p>为每对类别训练一个二分类器：</p><ul><li>对于K个类别，需要训练 $C_K^2 = \\frac{K(K-1)}{2}$ 个分类器</li><li>预测时，采用投票机制选择得票最多的类别</li></ul><h3 id="softmax回归" tabindex="-1">Softmax回归 <a class="header-anchor" href="#softmax回归" aria-label="Permalink to &quot;Softmax回归&quot;">​</a></h3><p>Softmax回归是逻辑回归在多分类问题上的直接推广：</p><p>$$P(y=k|x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}$$</p><p>其中：</p><ul><li>$K$ 是类别数量</li><li>$\\theta_k$ 是第k类的参数向量</li><li>分母是归一化项，确保所有类别的概率和为1</li></ul><p><strong>Softmax函数特性：</strong></p><ul><li>输出是概率分布（所有值在[0,1]之间，且和为1）</li><li>当K=2时，Softmax回归等价于逻辑回归</li></ul><h2 id="模型评估指标" tabindex="-1">模型评估指标 <a class="header-anchor" href="#模型评估指标" aria-label="Permalink to &quot;模型评估指标&quot;">​</a></h2><h3 id="混淆矩阵" tabindex="-1">混淆矩阵 <a class="header-anchor" href="#混淆矩阵" aria-label="Permalink to &quot;混淆矩阵&quot;">​</a></h3><p>二分类问题的混淆矩阵包含四个指标：</p><table tabindex="0"><thead><tr><th></th><th>预测为正类</th><th>预测为负类</th></tr></thead><tbody><tr><td><strong>实际为正类</strong></td><td>TP (真正例)</td><td>FN (假负例)</td></tr><tr><td><strong>实际为负类</strong></td><td>FP (假正例)</td><td>TN (真负例)</td></tr></tbody></table><h3 id="常用评估指标" tabindex="-1">常用评估指标 <a class="header-anchor" href="#常用评估指标" aria-label="Permalink to &quot;常用评估指标&quot;">​</a></h3><p><strong>准确率（Accuracy）：</strong></p><p>$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$</p><p>所有预测正确的样本占总样本的比例。</p><p><strong>精确率（Precision）：</strong></p><p>$$\\text{Precision} = \\frac{TP}{TP + FP}$$</p><p>预测为正类的样本中，实际为正类的比例。</p><p><strong>召回率（Recall，灵敏度）：</strong></p><p>$$\\text{Recall} = \\frac{TP}{TP + FN}$$</p><p>实际为正类的样本中，被正确预测为正类的比例。</p><p><strong>F1分数：</strong></p><p>$$\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$</p><p>精确率和召回率的调和平均数，平衡两者。</p><p><strong>特异性（Specificity）：</strong></p><p>$$\\text{Specificity} = \\frac{TN}{TN + FP}$$</p><p>实际为负类的样本中，被正确预测为负类的比例。</p><h3 id="roc曲线和auc" tabindex="-1">ROC曲线和AUC <a class="header-anchor" href="#roc曲线和auc" aria-label="Permalink to &quot;ROC曲线和AUC&quot;">​</a></h3><p><strong>ROC曲线（Receiver Operating Characteristic）：</strong></p><ul><li>横轴：假正例率（FPR）= $\\frac{FP}{FP + TN}$</li><li>纵轴：真正例率（TPR）= $\\frac{TP}{TP + FN}$ = Recall</li><li>通过改变分类阈值，得到不同的(FPR, TPR)点，连接成曲线</li></ul><p><strong>AUC（Area Under Curve）：</strong></p><ul><li>ROC曲线下的面积</li><li>取值范围：[0, 1]</li><li>AUC = 1：完美分类器</li><li>AUC = 0.5：随机分类器</li><li>AUC越大，模型性能越好</li></ul><h3 id="如何选择评估指标" tabindex="-1">如何选择评估指标？ <a class="header-anchor" href="#如何选择评估指标" aria-label="Permalink to &quot;如何选择评估指标？&quot;">​</a></h3><ul><li><strong>类别平衡</strong>：使用准确率</li><li><strong>关注正类</strong>：使用精确率和召回率</li><li><strong>类别不平衡</strong>：使用F1分数、AUC</li><li><strong>医疗诊断</strong>：通常更关注召回率（不能漏诊）</li><li><strong>垃圾邮件检测</strong>：通常更关注精确率（不能误判正常邮件）</li></ul><h2 id="优缺点" tabindex="-1">优缺点 <a class="header-anchor" href="#优缺点" aria-label="Permalink to &quot;优缺点&quot;">​</a></h2><h3 id="优点" tabindex="-1">优点 <a class="header-anchor" href="#优点" aria-label="Permalink to &quot;优点&quot;">​</a></h3><ol><li><strong>实现简单</strong>：算法原理清晰，易于理解和实现</li><li><strong>计算高效</strong>：训练和预测速度快，适合大规模数据</li><li><strong>可解释性强</strong>：可以解释每个特征对结果的影响（通过系数）</li><li><strong>不需要特征缩放</strong>：虽然缩放可能有助于优化，但不是必需的</li><li><strong>概率输出</strong>：直接输出概率值，便于后续决策</li><li><strong>不易过拟合</strong>：特别是使用正则化后</li><li><strong>在线学习</strong>：可以增量更新模型</li></ol><h3 id="缺点" tabindex="-1">缺点 <a class="header-anchor" href="#缺点" aria-label="Permalink to &quot;缺点&quot;">​</a></h3><ol><li><strong>线性决策边界</strong>：只能处理线性可分的分类问题</li><li><strong>特征工程重要</strong>：需要手动构造非线性特征（如多项式特征）</li><li><strong>对异常值敏感</strong>：异常值可能显著影响模型</li><li><strong>需要大量样本</strong>：在小样本情况下可能表现不佳</li><li><strong>假设特征独立</strong>：虽然不严格要求，但特征间高度相关可能影响性能</li></ol><h2 id="应用场景" tabindex="-1">应用场景 <a class="header-anchor" href="#应用场景" aria-label="Permalink to &quot;应用场景&quot;">​</a></h2><h3 id="典型应用领域" tabindex="-1">典型应用领域 <a class="header-anchor" href="#典型应用领域" aria-label="Permalink to &quot;典型应用领域&quot;">​</a></h3><ol><li><strong>医疗诊断</strong>：疾病预测、症状分析</li><li><strong>金融风控</strong>：信用评分、欺诈检测</li><li><strong>市场营销</strong>：客户流失预测、广告点击率预测</li><li><strong>自然语言处理</strong>：文本分类、情感分析</li><li><strong>推荐系统</strong>：用户行为预测</li><li><strong>图像识别</strong>：作为基础分类器（虽然现在更多用深度学习）</li></ol><h3 id="适用条件" tabindex="-1">适用条件 <a class="header-anchor" href="#适用条件" aria-label="Permalink to &quot;适用条件&quot;">​</a></h3><ul><li><strong>二分类或多分类问题</strong></li><li><strong>数据量适中到较大</strong></li><li><strong>特征与目标变量存在线性或近似线性关系</strong></li><li><strong>需要模型可解释性</strong></li><li><strong>对预测速度有要求</strong></li></ul><h2 id="与其他算法对比" tabindex="-1">与其他算法对比 <a class="header-anchor" href="#与其他算法对比" aria-label="Permalink to &quot;与其他算法对比&quot;">​</a></h2><h3 id="逻辑回归-vs-线性回归" tabindex="-1">逻辑回归 vs 线性回归 <a class="header-anchor" href="#逻辑回归-vs-线性回归" aria-label="Permalink to &quot;逻辑回归 vs 线性回归&quot;">​</a></h3><table tabindex="0"><thead><tr><th>特性</th><th>逻辑回归</th><th>线性回归</th></tr></thead><tbody><tr><td><strong>任务类型</strong></td><td>分类</td><td>回归</td></tr><tr><td><strong>输出</strong></td><td>概率值（0-1）</td><td>连续值</td></tr><tr><td><strong>激活函数</strong></td><td>Sigmoid</td><td>无（或恒等函数）</td></tr><tr><td><strong>损失函数</strong></td><td>交叉熵</td><td>均方误差</td></tr><tr><td><strong>输出解释</strong></td><td>类别概率</td><td>数值预测</td></tr></tbody></table><h3 id="逻辑回归-vs-支持向量机-svm" tabindex="-1">逻辑回归 vs 支持向量机（SVM） <a class="header-anchor" href="#逻辑回归-vs-支持向量机-svm" aria-label="Permalink to &quot;逻辑回归 vs 支持向量机（SVM）&quot;">​</a></h3><table tabindex="0"><thead><tr><th>特性</th><th>逻辑回归</th><th>SVM</th></tr></thead><tbody><tr><td><strong>决策边界</strong></td><td>线性</td><td>线性或非线性（核函数）</td></tr><tr><td><strong>输出</strong></td><td>概率</td><td>类别</td></tr><tr><td><strong>正则化</strong></td><td>L1/L2</td><td>L2（软间隔）</td></tr><tr><td><strong>计算复杂度</strong></td><td>低</td><td>中等</td></tr><tr><td><strong>大数据集</strong></td><td>适合</td><td>可能较慢</td></tr></tbody></table><h3 id="逻辑回归-vs-决策树" tabindex="-1">逻辑回归 vs 决策树 <a class="header-anchor" href="#逻辑回归-vs-决策树" aria-label="Permalink to &quot;逻辑回归 vs 决策树&quot;">​</a></h3><table tabindex="0"><thead><tr><th>特性</th><th>逻辑回归</th><th>决策树</th></tr></thead><tbody><tr><td><strong>可解释性</strong></td><td>系数解释</td><td>规则解释</td></tr><tr><td><strong>特征关系</strong></td><td>线性</td><td>非线性</td></tr><tr><td><strong>过拟合风险</strong></td><td>低（有正则化）</td><td>高</td></tr><tr><td><strong>特征重要性</strong></td><td>系数大小</td><td>信息增益等</td></tr></tbody></table><h2 id="实践建议" tabindex="-1">实践建议 <a class="header-anchor" href="#实践建议" aria-label="Permalink to &quot;实践建议&quot;">​</a></h2><h3 id="数据预处理" tabindex="-1">数据预处理 <a class="header-anchor" href="#数据预处理" aria-label="Permalink to &quot;数据预处理&quot;">​</a></h3><ol><li><strong>特征缩放</strong>：虽然不必须，但标准化或归一化有助于梯度下降收敛</li><li><strong>处理缺失值</strong>：填充、删除或使用特殊值</li><li><strong>特征选择</strong>：使用L1正则化或特征重要性分析</li><li><strong>处理类别不平衡</strong>：使用SMOTE、欠采样或调整类别权重</li></ol><h3 id="模型训练" tabindex="-1">模型训练 <a class="header-anchor" href="#模型训练" aria-label="Permalink to &quot;模型训练&quot;">​</a></h3><ol><li><strong>学习率选择</strong>：从0.01开始，根据收敛情况调整</li><li><strong>迭代次数</strong>：设置最大迭代次数，监控损失函数变化</li><li><strong>正则化参数</strong>：使用交叉验证选择最优 $\\lambda$</li><li><strong>阈值调整</strong>：根据业务需求调整分类阈值（不一定是0.5）</li></ol><h3 id="模型优化" tabindex="-1">模型优化 <a class="header-anchor" href="#模型优化" aria-label="Permalink to &quot;模型优化&quot;">​</a></h3><ol><li><strong>特征工程</strong>：构造多项式特征、交互特征</li><li><strong>集成方法</strong>：结合多个逻辑回归模型</li><li><strong>超参数调优</strong>：网格搜索或随机搜索</li><li><strong>交叉验证</strong>：使用K折交叉验证评估模型</li></ol><h3 id="常见问题与解决方案" tabindex="-1">常见问题与解决方案 <a class="header-anchor" href="#常见问题与解决方案" aria-label="Permalink to &quot;常见问题与解决方案&quot;">​</a></h3><h4 id="问题1-模型不收敛" tabindex="-1">问题1：模型不收敛 <a class="header-anchor" href="#问题1-模型不收敛" aria-label="Permalink to &quot;问题1：模型不收敛&quot;">​</a></h4><ul><li>检查学习率是否过大</li><li>检查特征是否已标准化</li><li>检查数据是否有异常值</li></ul><h4 id="问题2-过拟合" tabindex="-1">问题2：过拟合 <a class="header-anchor" href="#问题2-过拟合" aria-label="Permalink to &quot;问题2：过拟合&quot;">​</a></h4><ul><li>增加正则化强度</li><li>减少特征数量</li><li>增加训练数据量</li></ul><h4 id="问题3-欠拟合" tabindex="-1">问题3：欠拟合 <a class="header-anchor" href="#问题3-欠拟合" aria-label="Permalink to &quot;问题3：欠拟合&quot;">​</a></h4><ul><li>减少正则化强度</li><li>增加特征（包括多项式特征）</li><li>检查模型是否训练充分</li></ul><h4 id="问题4-类别不平衡" tabindex="-1">问题4：类别不平衡 <a class="header-anchor" href="#问题4-类别不平衡" aria-label="Permalink to &quot;问题4：类别不平衡&quot;">​</a></h4><ul><li>使用类别权重</li><li>使用SMOTE等技术</li><li>调整分类阈值</li></ul>',172)]))}const c=a(o,[["render",e]]);export{u as __pageData,c as default};
