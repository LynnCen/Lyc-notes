import{_ as a,c as t,o as s,a7 as r}from"./chunks/framework.BQb8NfN9.js";const m=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"ai/机器学习/08Kmeans.md","filePath":"ai/机器学习/08Kmeans.md","lastUpdated":1770393495000}'),n={name:"ai/机器学习/08Kmeans.md"};function l(i,e,o,_,p,c){return s(),t("div",null,e[0]||(e[0]=[r(`<p>一、机器学习数据探索聚类</p><ol><li>使用KMeans模型进行数据探索聚类 1）k-means算法介绍</li></ol><p>核心思想：根据样本间的相似性对样本集进行聚类，发现事物内部结构及相互关系 关键参数：需要预先指定聚类数量n_clusters（如2/3/4个簇），这是聚类的自主性特征 实际应用：工程师主要根据样本相似度发现数据内部结构，如案例中4个簇的数据也可分为3类 2）使用KMeans模型的构建过程 模型效果 原始数据按照2个簇聚类</p><p>数据准备： 使用make_blobs生成1000个样本，每个样本2个特征 设置4个质心位置：[-1,-1],[0,0],[1,1],[2,2] 标准差设为[0.4,0.2,0.2,0.2]，控制数据离散程度 固定随机种子random_state=22保证结果可复现 数据可视化： 通过plt.scatter绘制原始数据分布 x[:,0]表示第一个特征（横坐标），x[:,1]表示第二个特征（纵坐标） 实例化模型并预测</p><p>模型初始化： 关键参数n_clusters决定聚类数量（如2/3/4） init=&#39;k-means++&#39;优化质心初始化，默认尝试10次选择最优质心 random_state保持结果一致性 训练预测： 使用fit_predict()方法一步完成训练和预测 无监督学习不需要标签数据，直接输出聚类结果y_pred 展示聚类效果 结果可视化： 通过c=y_pred参数用不同颜色区分不同簇 每个点的颜色由其预测类别决定（如0/1/2/3对应不同颜色） 评估聚类效果好坏 评估指标： 使用calinski_harabasz_score评估聚类效果 分数越高表示聚类效果越好 比较不同n_clusters取值下的评分，选择最优聚类数 注意事项： 标准差参数影响数据离散程度，值越大数据越分散 特征矩阵x的列表示特征维度（案例中为2维坐标） k-means++初始化比随机初始化更稳定高效 二、无监督聚类K-means算法实现</p><ol><li>API基本使用流程</li></ol><p>数据准备：使用make_blobs生成测试数据 模型构建：通过KMeans类实现聚类 效果评估：采用calinski_harabasz_score指标 2. 数据创建与展示</p><p>关键参数： n_samples=1000：生成1000个样本 n_features=2：每个样本2个特征 centers=[[-1,-1],[0,0],[1,1],[2,2]]：4个聚类中心坐标 cluster_std=[0.4,0.2,0.2,0.2]：各簇标准差 可视化方法： plt.figure()创建画布 plt.scatter(x[:,0], x[:,1], marker=&#39;o&#39;)绘制散点图</p><h4 id="三、数据生成示例" tabindex="-1">三、数据生成示例 <a class="header-anchor" href="#三、数据生成示例" aria-label="Permalink to &quot;三、数据生成示例&quot;">​</a></h4><p>x, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1,-1],[0,0],[1,1],[2,2]], cluster_std=[0.4,0.2,0.2,0.2], random_state=11)</p><ol><li>模型实例化与预测</li></ol><p>核心参数： n_clusters：指定聚类数量（关键参数） init=&#39;k-means++&#39;：优化初始质心选择方法 n_init=&#39;auto&#39;：消除版本警告（1.4版本后默认值） 预测方法： fit_predict(x)：同时完成训练和预测 返回每个样本的聚类标签</p><h4 id="四、模型实例化与预测" tabindex="-1">四、模型实例化与预测 <a class="header-anchor" href="#四、模型实例化与预测" aria-label="Permalink to &quot;四、模型实例化与预测&quot;">​</a></h4><p>kmeans_cls = KMeans(n_clusters=2, init=&#39;k-means++&#39;, n_init=&#39;auto&#39;) y_pred = kmeans_cls.fit_predict(x)</p><ol><li>模型评估与可视化</li></ol><p>评估指标： calinski_harabasz_score(x, y_pred)：分数越高效果越好 可视化对比： 原始数据分布 vs 聚类结果分布 不同n_clusters参数的对比效果</p><h4 id="五、评估与可视化" tabindex="-1">五、评估与可视化 <a class="header-anchor" href="#五、评估与可视化" aria-label="Permalink to &quot;五、评估与可视化&quot;">​</a></h4><p>print(calinski_harabasz_score(x, y_pred)) plt.scatter(x[:,0], x[:,1], c=y_pred) plt.show()</p><ol><li>参数调整实践</li></ol><p>典型实验： n_clusters=2：CH分数3125.94 n_clusters=3：CH分数2964.31 n_clusters=4：CH分数5813.86（最优） 参数选择： 通过CH分数选择最佳聚类数 分数突然下降点往往是最佳k值 2. 完整代码结构</p><p>导入依赖包（sklearn.cluster, sklearn.datasets等） 创建测试数据集 可视化原始数据分布 实例化KMeans模型 进行聚类预测 可视化聚类结果 使用CH分数评估效果</p><h4 id="六、完整代码框架" tabindex="-1">六、完整代码框架 <a class="header-anchor" href="#六、完整代码框架" aria-label="Permalink to &quot;六、完整代码框架&quot;">​</a></h4><p>from sklearn.cluster import KMeans from sklearn.datasets import make_blobs import matplotlib.pyplot as plt</p><p>def kmeans_demo(): # 数据准备 x, y = make_blobs(...)</p><pre><code># 原始数据可视化
plt.scatter(...)

# 模型训练与预测
kmeans = KMeans(n_clusters=4)
y_pred = kmeans.fit_predict(x)

# 结果评估
print(calinski_harabasz_score(x, y_pred))
</code></pre><ol><li>注意事项 版本兼容：1.4版本后n_init默认值改为&#39;auto&#39;，需显式设置避免警告 数据标准化：不同特征量纲差异大时需先标准化 随机种子：设置random_state保证结果可复现 初始质心：k-means++方法优于随机初始化 七、知识小结 知识点核心内容考试重点/易混淆点难度系数K-means聚类算法原理通过迭代计算寻找最优聚类中心点，根据距离将数据点分配到不同类别初始中心点选择对结果影响大/需预先指定聚类数量⭐⭐⭐⭐聚类评估指标使用轮廓系数(Silhouette Coefficient)评估聚类效果，值越大表示聚类效果越好区分有监督和无监督评估方法⭐⭐⭐数据标准化处理通过标准差参数控制数据离散程度，影响聚类边界清晰度标准差设置与数据分布关系⭐⭐随机种子设置固定随机种子可确保实验可重复性，不设置则每次运行结果不同工程实践中的随机性控制⭐聚类可视化使用matplotlib展示不同聚类数量的效果对比特征维度与可视化映射关系⭐⭐无监督学习特点直接使用fit_predict方法，不需要标签数据与监督学习的fit+predict区别⭐⭐⭐多维数据处理通过n_features参数控制生成数据的特征维度高维数据可视化挑战⭐⭐⭐⭐算法参数调优n_init参数控制中心点初始化次数，默认10次参数优化与计算效率平衡⭐⭐⭐</li></ol>`,26)]))}const u=a(n,[["render",l]]);export{m as __pageData,u as default};
