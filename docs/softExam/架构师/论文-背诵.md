# 多站点AI智能创作一体化架构设计与实践

## 摘要

2024年1月，我发起稿定设计三站一体化重构，主要负责国内主战、AI创新社区、海外Insmind，并在2025年3月完成上线。我作为项目负责人和系统架构师，主要负责总体架构与落地，将AI能力贯穿“找素材、做设计、出成品、投分发、看效果”的端到端链路，让不会设计的人也能一步完成高质量创作，采用分层+微服务+事件驱动的混合架构模式，前端以monorepo（WorkSapce+Turbo）与SSR/同构配合多级缓存，后端以API GatWay+BFF+gRPC配合Outbox+CDC保证一致性，AI能力通过Dify私有化与Agent工程化实现多模型编排与RAG增强。我聚焦性能、可用性、安全与合规与可维护性四类质量目标。系统上线后，P95延迟、首屏时长、峰值QPS、导出成功率、SLA、推理成本与人效等关键指标显著改善，AI+能力沉淀为公司产品迭代与生态增长的新基座。系统达成“让不会设计的人一步出图”，支撑业务增长与全球化。

## 项目背景与目标

我置身于数字内容与营销高速增长的行业周期，看到非设计人群已经成为内容的主要生产力。以往的工具链设置了高门槛，协作链路冗长且不可视；跨团队、跨站点的品牌规范与体验很难保持一致；同时“海量模版检索、跨端适配与全球化分发”的性能问题，与“生成式AI在合规与成本约束下的落地”问题在同一套业务流程中叠加，直接影响转化、口碑与增长。我以统一架构承载国内主站、AI创新社区与海外Insmind的一致体验，把“模版+AI工具链”贯穿从灵感到成品的端到端流程。在目标上，我把系统定位为“三高一稳一可观测”。我亲自牵引跨职能团队，项目团队由多名角色协同组成：架构师与技术负责人统筹方案与节奏，前端工程师负责多站同构、组件与性能优化，后端工程师负责领域服务与接口契约治理，AI平台工程师负责模型接入、工作流与Agent工程化，搜索与数据工程师负责检索，向量索引与数据治理，测试与SRE负责质量保证与发布，设计与产品负责体验场景落地，内容与合规团队保证素材版权与风控。以敏捷迭代与CI/CD落地从需求到上线的闭环。

## 需求分析

在需求分析阶段，我把主干流程抽象为“模版检索 -> 在线编辑 -> AI生成/改图 -> 导出分发”。海量模版与素材需要在显示字段、标签与向量特征上实现多维检索与相似度召回；在线编辑必须覆盖图/文/视频/H5等形态，并提供无障碍的一键导出；AI侧既要提供文生图、图生图等内容生成，也要提供抠图、扩图、消除与对话式作图等增强能力；导出与分发要通过对象存储与CDN服务跨区域低时延抵达。

在非功能约束下，我把高并发、低时延与弹性能力放在首位，要求关键链路具备熔断、降级与回退重试等策略；上线流程坚持灰度、金丝雀与可回滚；安全与合规以最小权利、传输与存储加密、审计留痕与内容风控落地到每一条链路。围绕这些约束，我在DFD与ER视图中确立“用户 - 项目 - 页面 - 涂层- 素材- 导出结果”这条核心实体主线，并以统一接口承接三站入口，避免前端直接多服务导致的耦合与抖动。

## 总体架构设计

我采用“分层 + 微服务 + 事件驱动”的混合架构，以应对跨站一致性、服务耦合与 AI 长耗时任务的挑战。在分层设计上，我将系统划分为明确的层次：最上层是用户体验层，通过 Monorepo 与 SSR/同构技术保障毫秒级首屏；其下是业务边界层，设立 API Gateway 作为流量入口，并为多端设立轻量 BFF 聚合服务；核心服务层是架构的主体，我基于领域驱动设计（DDD）原则，将其拆分为认证授权、模板素材、在线编辑、资产网关、计费订单及 AI 编排等高内聚微服务，服务间采用高性能 gRPC 通信，契约由 OpenAPI/Protobuf 管理。AI 编排服务作为关键枢纽，通过私有化 Dify 平台封装多模型能力与 RAG 知识增强。为解耦这些服务并保障数据一致性，我引入了事件驱动机制，采用“事务性发件箱 + CDC”模式将领域事件可靠发布至消息总线，由下游幂等消费，以此处理所有长耗时任务并取代分布式事务。整个架构运行在以 Kubernetes 为核心的基础设施之上，通过 CI/CD 实现灰度发布，并由 OpenTelemetry 三件套提供全链路可观测性，确保系统的透明与可控。

## 详细设计

在详细设计与实现阶段，我全面推行敏捷 Scrum 方法论，以双周为迭代周期，通过每日站会快速对齐需求颗粒度与风险，并协同前端、后端、AI 及算法团队分工推进。前端团队负责解决多站复用与性能问题，他们采用 Monorepo（pnpm Workspaces+Turborepo）统一代码与并行构建，结合 SSR/同构与路由级代码分割优化首屏，并以原子化样式与设计令牌确保视觉一致性，通过 Storybook 与 Playwright 等工具链保障体验质量。后端团队基于 DDD 思想划分服务边界，明确了模板、编辑、资产、计费等领域，内部服务间采用高性能 gRPC 通信，对外则提供 REST/JSON 接口；他们通过契约测试守护接口稳定，以 OAuth2/OIDC 与 RBAC/ABAC 落实安全，并借助熔断、限流、重试等韧性模式应对流量冲击，同时将 AI 生成、导出等长耗时任务全面异步化，通过 Kafka/RabbitMQ 消息队列与幂等消费提升系统吞吐与稳定性。AI 团队专注于将大模型能力工程化，他们通过私有化 Dify 平台封装多模型（豆包、GPT）的复杂交互，建立包含 RAG 知识增强与工具链的 Agent 工作流，并设置防腐层以隔离外部变化。算法团队则致力于优化核心转化路径，他们负责优化 Elasticsearch 的多维检索与排序模型，并基于用户行为数据构建模板推荐引擎，通过 A/B 测试持续迭代策略。

在测试阶段，为保障系统质量与稳定运行，我构建了从测试到运维的自动化闭环，我推行测试金字塔策略，通过单元、集成、契约及端到端（E2E）测试覆盖关键业务路径，并以性能压测建立容量基线。发布流程严格遵循“评审-演练-灰度-观测-回滚”的规范：所有变更需经预生产环境演练，采用灰度或金丝雀模式逐步放量，通过可观测数据验证后，再切换至蓝绿部署，并始终保持一键回滚能力。在运维层面，系统韧性通过混沌工程与常态化故障演练持续加固，配合限流、熔断、降级等预案，确保在大促等极端场景下的服务连续性。同时，我强化了供应链安全，实施 SBOM 与镜像扫描，并通过集中化的配置与密钥管理降低风险。最终，由日志、指标、链路追踪构成的统一可观测平台，为告警分级、应急响应及快速恢复提供了坚实的数据基础。

## 效果与展望

从性能维度看，我把 TTFB 压到约 200ms，首屏控制在 1s 左右，接口 P95/P99 分别稳定在 150ms/300ms；借助异步编排与缓存/批处理，峰值 QPS 提升到原来的两到三倍，AI 生成链路可见结果时间（P95）降至约 3s。在可用性维度，SLA 稳定在 99.95% 以上，熔断与降级把故障影响半径收紧，蓝绿/一键回滚让 MTTR 达到分钟级，导出成功率提升到 99.5% 以上。

这次重构从问题出发，以“分层 + 微服务 + 事件驱动”为骨架，串联 Monorepo+SSR 的前端体验优化、API Gateway+BFF 的接口治理、Outbox+CDC 的一致性保障、Dify+Agent 的 AI 编排与 RAG 强化，以及覆盖开发—发布—运维全链路的可观测体系。我用小步快跑与可回滚的工程纪律，让复杂系统的演进变成一组可度量、可追溯的变化，最终达成性能、可用性、安全与效率的多目标平衡。

接下来，我会在架构层面继续做域内去耦与能力平台化，优化 BFF 的聚合策略与契约演进；在 AI 能力上引入更深的多模态工具链与离线评测/在线学习闭环，持续优化质量—成本曲线；在数据与检索上推进更细粒度的画像索引与在线增量重建，提升向量召回与一致性；在工程与运维上完善 SLO/SLA 体系与混沌演练常态化，进一步提高自动回滚的决策效率；在合规与安全上细化地域化策略，完善水印与溯源方案，夯实供应链安全基线。通过持续的工程化与运营数据驱动，我希望把“让不会设计的人一步出图”从愿景变成稳定可复制的能力。
