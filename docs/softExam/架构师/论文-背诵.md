# 多站点AI智能创作一体化架构设计与实践

## 摘要

2024年1月，我发起稿定设计三站一体化重构，主要负责国内主战、AI创新社区、海外Insmind，并在2025年3月完成上线。我作为项目负责人和系统架构师，主要负责总体架构与落地，将AI能力贯穿“找素材、做设计、出成品、投分发、看效果”的端到端链路，让不会设计的人也能一步完成高质量创作，采用分层+微服务+事件驱动的混合架构模式，前端以monorepo（WorkSapce+Turbo）与SSR/同构配合多级缓存，后端以API GatWay+BFF+gRPC配合Outbox+CDC保证一致性，AI能力通过Dify私有化与Agent工程化实现多模型编排与RAG增强。我聚焦性能、可用性、安全与合规与可维护性四类质量目标。系统上线后，P95延迟、首屏时长、峰值QPS、导出成功率、SLA、推理成本与人效等关键指标显著改善，AI+能力沉淀为公司产品迭代与生态增长的新基座。系统达成“让不会设计的人一步出图”，支撑业务增长与全球化。

## 项目背景与目标

我置身于数字内容与营销高速增长的行业周期，看到非设计人群已经成为内容的主要生产力。以往的工具链设置了高门槛，协作链路冗长且不可视；跨团队、跨站点的品牌规范与体验很难保持一致；同时“海量模版检索、跨端适配与全球化分发”的性能问题，与“生成式AI在合规与成本约束下的落地”问题在同一套业务流程中叠加，直接影响转化、口碑与增长。我以统一架构承载国内主站、AI创新社区与海外Insmind的一致体验，把“模版+AI工具链”贯穿从灵感到成品的端到端流程。在目标上，我把系统定位为“三高一稳一可观测”。我亲自牵引跨职能团队，项目团队由多名角色协同组成：架构师与技术负责人统筹方案与节奏，前端工程师负责多站同构、组件与性能优化，后端工程师负责领域服务与接口契约治理，AI平台工程师负责模型接入、工作流与Agent工程化，搜索与数据工程师负责检索，向量索引与数据治理，测试与SRE负责质量保证与发布，设计与产品负责体验场景落地，内容与合规团队保证素材版权与风控。以敏捷迭代与CI/CD落地从需求到上线的闭环。

## 需求分析

在需求分析阶段，我把主干流程抽象为“模版检索 -> 在线编辑 -> AI生成/改图 -> 导出分发”。海量模版与素材需要在显示字段、标签与向量特征上实现多维检索与相似度召回；在线编辑必须覆盖图/文/视频/H5等形态，并提供无障碍的一键导出；AI侧既要提供文生图、图生图等内容生成，也要提供抠图、扩图、消除与对话式作图等增强能力；导出与分发要通过对象存储与CDN服务跨区域低时延抵达。

在非功能约束下，我把高并发、低时延与弹性能力放在首位，要求关键链路具备熔断、降级与回退重试等策略；上线流程坚持灰度、金丝雀与可回滚；安全与合规以最小权利、传输与存储加密、审计留痕与内容风控落地到每一条链路。围绕这些约束，我在DFD与ER视图中确立“用户 - 项目 - 页面 - 涂层- 素材- 导出结果”这条核心实体主线，并以统一接口承接三站入口，避免前端直接多服务导致的耦合与抖动。

## 总体架构设计

我采用“分层 + 微服务 + 事件驱动”的混合架构，以应对跨站一致性、服务耦合与 AI 长耗时任务的挑战。在分层设计上，我将系统划分为明确的层次：最上层是用户体验层，通过 Monorepo 与 SSR/同构技术保障毫秒级首屏；其下是业务边界层，设立 API Gateway 作为流量入口，并为多端设立轻量 BFF 聚合服务；核心服务层是架构的主体，我基于领域驱动设计（DDD）原则，将其拆分为认证授权、模板素材、在线编辑、资产网关、计费订单及 AI 编排等高内聚微服务，服务间采用高性能 gRPC 通信，契约由 OpenAPI/Protobuf 管理。AI 编排服务作为关键枢纽，通过私有化 Dify 平台封装多模型能力与 RAG 知识增强。为解耦这些服务并保障数据一致性，我引入了事件驱动机制，采用“事务性发件箱 + CDC”模式将领域事件可靠发布至消息总线，由下游幂等消费，以此处理所有长耗时任务并取代分布式事务。整个架构运行在以 Kubernetes 为核心的基础设施之上，通过 CI/CD 实现灰度发布，并由 OpenTelemetry 三件套提供全链路可观测性，确保系统的透明与可控。

## 详细设计

在详细设计与实现阶段，我全面推行敏捷 Scrum 方法论，以双周为迭代周期，通过每日站会快速对齐需求颗粒度与风险，并协同前端、后端、AI 及算法团队分工推进。前端团队负责解决多站复用与性能问题，他们采用 Monorepo（pnpm Workspaces+Turborepo）统一代码与并行构建，结合 SSR/同构与路由级代码分割优化首屏，并以原子化样式与设计令牌确保视觉一致性，通过 Storybook 与 Playwright 等工具链保障体验质量。后端团队基于 DDD 思想划分服务边界，明确了模板、编辑、资产、计费等领域，内部服务间采用高性能 gRPC 通信，对外则提供 REST/JSON 接口；他们通过契约测试守护接口稳定，以 OAuth2/OIDC 与 RBAC/ABAC 落实安全，并借助熔断、限流、重试等韧性模式应对流量冲击，同时将 AI 生成、导出等长耗时任务全面异步化，通过 Kafka/RabbitMQ 消息队列与幂等消费提升系统吞吐与稳定性。AI 团队专注于将大模型能力工程化，他们通过私有化 Dify 平台封装多模型（豆包、GPT）的复杂交互，建立包含 RAG 知识增强与工具链的 Agent 工作流，并设置防腐层以隔离外部变化。算法团队则致力于优化核心转化路径，他们负责优化 Elasticsearch 的多维检索与排序模型，并基于用户行为数据构建模板推荐引擎，通过 A/B 测试持续迭代策略。

在测试阶段，为保障系统质量与稳定运行，我构建了从测试到运维的自动化闭环，我推行测试金字塔策略，通过单元、集成、契约及端到端（E2E）测试覆盖关键业务路径，并以性能压测建立容量基线。发布流程严格遵循“评审-演练-灰度-观测-回滚”的规范：所有变更需经预生产环境演练，采用灰度或金丝雀模式逐步放量，通过可观测数据验证后，再切换至蓝绿部署，并始终保持一键回滚能力。在运维层面，系统韧性通过混沌工程与常态化故障演练持续加固，配合限流、熔断、降级等预案，确保在大促等极端场景下的服务连续性。同时，我强化了供应链安全，实施 SBOM 与镜像扫描，并通过集中化的配置与密钥管理降低风险。最终，由日志、指标、链路追踪构成的统一可观测平台，为告警分级、应急响应及快速恢复提供了坚实的数据基础。

## 效果与展望

从性能维度看，我把 TTFB 压到约 200ms，首屏控制在 1s 左右，接口 P95/P99 分别稳定在 150ms/300ms；借助异步编排与缓存/批处理，峰值 QPS 提升到原来的两到三倍，AI 生成链路可见结果时间（P95）降至约 3s。在可用性维度，SLA 稳定在 99.95% 以上，熔断与降级把故障影响半径收紧，蓝绿/一键回滚让 MTTR 达到分钟级，导出成功率提升到 99.5% 以上。

这次重构从问题出发，以“分层 + 微服务 + 事件驱动”为骨架，串联 Monorepo+SSR 的前端体验优化、API Gateway+BFF 的接口治理、Outbox+CDC 的一致性保障、Dify+Agent 的 AI 编排与 RAG 强化，以及覆盖开发—发布—运维全链路的可观测体系。我用小步快跑与可回滚的工程纪律，让复杂系统的演进变成一组可度量、可追溯的变化，最终达成性能、可用性、安全与效率的多目标平衡。

接下来，我会在架构层面继续做域内去耦与能力平台化，优化 BFF 的聚合策略与契约演进；在 AI 能力上引入更深的多模态工具链与离线评测/在线学习闭环，持续优化质量—成本曲线；在数据与检索上推进更细粒度的画像索引与在线增量重建，提升向量召回与一致性；在工程与运维上完善 SLO/SLA 体系与混沌演练常态化，进一步提高自动回滚的决策效率；在合规与安全上细化地域化策略，完善水印与溯源方案，夯实供应链安全基线。通过持续的工程化与运营数据驱动，我希望把“让不会设计的人一步出图”从愿景变成稳定可复制的能力。

# 技术要点解析

## 一、前端架构与体验优化

### 1.1 Monorepo（单一代码仓库）

**定义**：将多个相关项目（国内主站、AI社区、海外Insmind）放在同一个Git仓库中管理的代码组织方式。

**解决的问题**：多站点代码重复、依赖版本不一致、跨项目协作困难。

**项目应用**：通过pnpm Workspaces管理依赖，结合Turborepo实现增量构建和并行打包，让三个站点共享组件库、工具函数和类型定义，避免重复开发。

### 1.2 Workspaces（工作区）

**定义**：包管理工具（如pnpm、npm、yarn）提供的多包管理功能，允许在同一仓库中管理多个子项目。

**解决的问题**：子项目间依赖安装冗余、版本冲突、本地调试困难。

**项目应用**：统一管理三站依赖，共享node_modules，实现本地包互相引用而无需发布到npm。

### 1.3 Turborepo

**定义**：高性能的Monorepo构建系统，提供智能缓存和任务编排能力。

**解决的问题**：Monorepo中构建速度慢、重复构建未变更的包。

**项目应用**：通过任务依赖图并行构建，缓存未变更的构建产物，将整体构建时间从数十分钟缩短到几分钟。

### 1.4 SSR/同构（Server-Side Rendering/Isomorphic）

**定义**：在服务器端预渲染页面HTML，同时保持客户端交互能力的技术。同构指同一套代码可在服务端和客户端运行。

**解决的问题**：SPA应用首屏白屏时间长、SEO不友好、首次交互延迟高。

**项目应用**：模板检索页在服务端渲染首屏内容，TTFB控制在200ms，首屏时间约1s，同时保证良好的SEO效果。

### 1.5 多级缓存

**定义**：在不同层级（浏览器、CDN、网关、应用、数据库）设置缓存，减少请求链路和响应时间。

**解决的问题**：海量模板和素材的重复查询导致后端压力大、响应慢。

**项目应用**：热门模板数据在CDN缓存1小时，用户项目在Redis缓存15分钟，配合浏览器LocalStorage实现毫秒级响应。

### 1.6 路由级代码分割（Route-based Code Splitting）

**定义**：按照路由将应用代码拆分成多个chunk，按需加载的优化技术。

**解决的问题**：单个bundle过大导致首次加载慢。

**项目应用**：将编辑器、导出、AI工具等模块独立打包，首屏只加载必要代码，首屏JS体积从2MB降至300KB。

### 1.7 原子化样式（Atomic CSS）

**定义**：使用单一用途的CSS类（如`m-4`、`text-center`）来构建界面的样式方法。

**解决的问题**：CSS冗余、命名冲突、样式覆盖混乱。

**项目应用**：采用Tailwind CSS等工具，三站共享样式原子，保证视觉一致性，生产环境通过PurgeCSS清除未使用样式，CSS体积减少80%。

### 1.8 设计令牌（Design Tokens）

**定义**：将设计系统中的颜色、间距、字体等设计决策抽象为变量，统一管理和复用。

**解决的问题**：多站点品牌风格不一致、样式硬编码难以维护。

**项目应用**：定义全局颜色、字体、圆角等变量，通过配置文件实现三站主题切换，保证品牌一致性。

## 二、后端架构与服务治理

### 2.1 分层架构（Layered Architecture）

**定义**：将系统按职责划分为展示层、业务层、数据层等水平层次，层间依赖单向的架构模式。

**解决的问题**：代码职责不清、耦合严重、难以测试和维护。

**项目应用**：分为用户体验层（SSR）→业务边界层（Gateway/BFF）→核心服务层（微服务）→基础设施层（K8s/DB），每层职责清晰。

### 2.2 微服务架构（Microservices Architecture）

**定义**：将单体应用拆分为多个独立部署、独立扩展的小型服务，每个服务负责一个业务能力。

**解决的问题**：单体应用难以扩展、故障影响全局、技术栈固化。

**项目应用**：拆分为认证授权、模板素材、在线编辑、资产网关、计费订单、AI编排等6+个微服务，独立部署和扩容。

### 2.3 事件驱动架构（Event-Driven Architecture, EDA）

**定义**：通过异步事件在服务间传递状态变化，实现松耦合通信的架构模式。

**解决的问题**：同步调用链路长导致超时、服务间强依赖、分布式事务难实现。

**项目应用**：AI生成、导出、计费等长耗时操作通过事件异步处理，主流程立即返回，提升系统吞吐量2-3倍。

### 2.4 API Gateway（API网关）

**定义**：系统的统一入口，负责路由、认证、限流、监控等横切关注点。

**解决的问题**：前端直连多个微服务导致复杂性高、安全风险大、流量无法统一管控。

**项目应用**：所有外部请求经过Gateway，统一处理鉴权、限流（10000 QPS）、日志、熔断，隐藏后端服务细节。

### 2.5 BFF（Backend for Frontend，服务于前端的后端）

**定义**：为不同前端（Web、Mobile、小程序）定制的轻量聚合层，按前端需求组装数据。

**解决的问题**：前端直接调用多个微服务导致请求多、数据冗余、网络开销大。

**项目应用**：为Web端、移动端分别设立BFF，一次请求聚合模板、用户、权益等多个服务数据，减少80%的前端请求数。

### 2.6 gRPC

**定义**：Google开源的高性能RPC框架，基于HTTP/2和Protocol Buffers，支持多语言。

**解决的问题**：REST/JSON通信性能低、序列化开销大、接口契约不明确。

**项目应用**：微服务间通信采用gRPC，比REST快5-10倍，通过.proto文件定义接口契约，实现类型安全和自动代码生成。

### 2.7 领域驱动设计（Domain-Driven Design, DDD）

**定义**：一种软件设计方法论,通过识别业务领域、建立统一语言、划分限界上下文来组织复杂系统。

**解决的问题**：业务逻辑分散、模型贫血、服务边界模糊。

**项目应用**：识别"模板"、"编辑"、"资产"、"订单"等领域，每个微服务对应一个限界上下文，内部采用聚合根、实体、值对象等DDD战术模式。

### 2.8 契约测试（Contract Testing）

**定义**：验证服务间接口约定（契约）是否被正确实现和遵守的测试方法。

**解决的问题**：微服务间接口变更导致运行时错误、集成测试成本高。

**项目应用**：使用Pact等工具，生产者发布契约，消费者验证契约，在CI阶段提前发现接口不兼容问题。

### 2.9 REST/JSON

**定义**：基于HTTP协议，使用JSON格式传输数据的API设计风格。

**解决的问题**：为外部客户端（浏览器、第三方）提供易用的标准化接口。

**项目应用**：对外暴露的API采用RESTful风格，易于前端和第三方集成。

## 三、数据一致性与异步处理

### 3.1 Outbox Pattern（事务性发件箱模式）

**定义**：在业务事务中同时写入业务数据和事件到同一数据库，通过后续进程保证事件发布的模式。

**解决的问题**：分布式事务复杂、本地事务与消息发送无法保证原子性。

**项目应用**：用户导出时，在同一事务中写入"导出记录"和"导出事件"到数据库，再由CDC同步到消息队列，保证事件100%发布。

### 3.2 CDC（Change Data Capture，变更数据捕获）

**定义**：监听数据库变更日志（如MySQL binlog），实时捕获数据变化并发布到下游的技术。

**解决的问题**：Outbox表中的事件如何可靠、低延迟地发送到消息队列。

**项目应用**：使用Debezium监听Outbox表的Insert事件,实时（毫秒级）推送到Kafka，实现最终一致性。

### 3.3 消息总线/队列（Kafka/RabbitMQ）

**定义**：支持发布订阅、解耦、削峰填谷的分布式消息中间件。

**解决的问题**：同步调用导致服务耦合、流量突刺导致系统崩溃。

**项目应用**：Kafka作为事件总线，承载导出、AI生成、计费等异步事件，支持百万级QPS，配合消费者组实现负载均衡和容错。

### 3.4 幂等消费（Idempotent Consumer）

**定义**：消费者处理消息时,保证重复消费同一条消息不会产生副作用的设计。

**解决的问题**：消息队列至少一次语义导致重复消费引发数据重复或错误。

**项目应用**：通过消息ID + 分布式锁或唯一索引，保证导出、扣费等操作幂等，即使消息重复也不会重复扣费或生成多个文件。

## 四、AI与算法工程化

### 4.1 Dify

**定义**：开源的LLMOps（大语言模型运维）平台，提供工作流编排、提示词管理、模型接入等能力。

**解决的问题**：直接调用大模型API复杂、提示词难以版本化管理、多模型切换困难。

**项目应用**：私有化部署Dify，封装豆包、GPT等多个模型，通过可视化工作流编排"理解需求→生成方案→调用工具→返回结果"流程，提升AI能力复用性。

### 4.2 Agent（AI代理）

**定义**：能够自主规划、调用工具、迭代执行任务的AI智能体。

**解决的问题**：单次模型调用无法完成复杂任务，需要人工拆解和多轮交互。

**项目应用**：实现"对话式作图Agent"，用户输入需求后，Agent自动分解为"理解意图→检索模板→生成素材→布局排版→预览反馈"多步骤，自动调用编辑器API完成创作。

### 4.3 RAG（Retrieval-Augmented Generation，检索增强生成）

**定义**：结合向量检索和大模型生成，先从知识库检索相关信息，再输入模型生成答案的技术。

**解决的问题**：大模型幻觉问题、无法利用实时/私有知识。

**项目应用**：将历史模板、设计规范、品牌素材向量化存入知识库，AI生成前先检索相似案例，提升生成质量并降低幻觉，使生成结果更符合品牌风格。

### 4.4 Elasticsearch（ES）

**定义**：基于Lucene的分布式搜索引擎，支持全文检索、聚合分析、近实时搜索。

**解决的问题**：海量模板的多维检索（标签、类别、风格）和排序（热度、相似度）性能低。

**项目应用**：千万级模板索引在ES中，支持文本、标签、向量特征的混合检索，P95响应时间<100ms，配合排序模型提升召回精度。

### 4.5 向量索引/向量检索

**定义**：将图像、文本转换为高维向量（Embedding），通过向量相似度（余弦、欧氏距离）检索相似内容的技术。

**解决的问题**：传统关键词检索无法理解语义、找不到"相似风格"的模板。

**项目应用**：模板图片通过CLIP等模型生成向量，存储在ES或专用向量库（Milvus），用户上传参考图可检索相似模板，提升"以图搜模板"的准确率。

### 4.6 A/B测试

**定义**：将用户随机分组,对比不同版本（策略、UI、算法）效果的实验方法。

**解决的问题**：不确定新功能、新算法是否真正提升转化率。

**项目应用**：搜索排序策略、推荐算法、AI生成参数通过A/B测试持续优化，用数据驱动决策，最终使模板点击率提升15%。

### 4.7 多模态

**定义**：同时处理和融合文本、图像、语音、视频等多种数据类型的AI能力。

**解决的问题**：单一模态（仅文本或仅图像）无法满足复杂创作需求。

**项目应用**：用户可输入"文字描述+参考图+风格关键词"，系统融合多模态信息生成更精准的设计结果。

## 五、DevOps与SRE实践

### 5.1 CI/CD（Continuous Integration/Continuous Deployment，持续集成/持续部署）

**定义**：自动化构建、测试、部署流程，频繁将代码集成到主干并快速发布到生产环境的实践。

**解决的问题**：手动发布慢且易错、集成冲突发现晚、反馈周期长。

**项目应用**：代码提交后自动触发单元测试→集成测试→构建镜像→推送仓库→灰度发布，从提交到上线缩短到30分钟。

### 5.2 Kubernetes（K8s）

**定义**：开源的容器编排平台，自动化部署、扩展和管理容器化应用。

**解决的问题**：手动管理容器复杂、服务发现困难、弹性伸缩慢。

**项目应用**：所有微服务容器化并运行在K8s集群，支持自动扩缩容（HPA），峰值时AI服务从10个Pod自动扩到50个，保证可用性。

### 5.3 OpenTelemetry

**定义**：CNCF开源的可观测性框架，统一收集Trace（链路）、Metrics（指标）、Logs（日志）的标准。

**解决的问题**：各服务使用不同监控工具导致数据割裂、故障排查困难。

**项目应用**：所有服务集成OpenTelemetry SDK，统一上报到Jaeger（链路）、Prometheus（指标）、Loki（日志），实现从前端到后端的全链路追踪。

### 5.4 灰度发布/金丝雀发布（Canary Deployment）

**定义**：将新版本先发布到小部分用户（如5%），观察无问题后逐步扩大范围的发布策略。

**解决的问题**：全量发布风险高，新版本bug影响所有用户。

**项目应用**：新功能先灰度到1%用户,观测错误率、延迟等指标正常后，逐步放量到10%→50%→100%，异常时一键回滚。

### 5.5 蓝绿部署（Blue-Green Deployment）

**定义**：维护两套完全相同的生产环境（蓝/绿），部署新版本到绿环境，切流量到绿环境，蓝环境作为回退。

**解决的问题**：发布中断服务、回滚慢。

**项目应用**：编辑器等核心服务采用蓝绿部署，流量瞬间切换，回滚只需切回流量，MTTR降至2分钟。

### 5.6 测试金字塔（Test Pyramid）

**定义**：由大量单元测试（底层）→适量集成测试（中层）→少量端到端测试（顶层）组成的测试策略。

**解决的问题**：过度依赖手工测试或E2E测试导致反馈慢、维护成本高。

**项目应用**：70%单元测试覆盖业务逻辑，20%集成测试覆盖服务间交互，10%E2E测试覆盖关键用户路径，在CI中15分钟跑完全部测试。

### 5.7 性能压测（Performance Testing）

**定义**：模拟大量并发用户或请求，测试系统在高负载下的性能表现和瓶颈。

**解决的问题**：不清楚系统容量上限、生产环境突发流量导致崩溃。

**项目应用**：使用JMeter模拟10万并发用户，建立容量基线（单机支撑2000 QPS），指导弹性伸缩策略和容量规划。

### 5.8 混沌工程（Chaos Engineering）

**定义**：主动在生产环境注入故障（如杀进程、网络延迟），验证系统韧性的实践。

**解决的问题**：不确定系统在真实故障场景下能否正常运行。

**项目应用**：每月进行混沌演练，随机停止某个服务实例，验证熔断、降级、重试策略有效性，确保单点故障不影响整体可用性。

### 5.9 SBOM（Software Bill of Materials，软件物料清单）

**定义**：列出软件中所有依赖组件及其版本的清单，类似工业产品的BOM。

**解决的问题**：不清楚使用了哪些三方库、是否存在已知漏洞。

**项目应用**：构建时自动生成SBOM，通过Trivy等工具扫描依赖漏洞，发现Log4j等高危漏洞时立即修复，降低供应链攻击风险。

### 5.10 镜像扫描（Image Scanning）

**定义**：自动检测Docker镜像中的安全漏洞、恶意代码、配置错误的工具。

**解决的问题**：容器镜像可能包含过期软件包或恶意后门。

**项目应用**：镜像推送到Harbor仓库时自动触发Clair扫描，发现高危漏洞阻止部署，确保只有安全镜像上线。

### 5.11 熔断（Circuit Breaker）

**定义**：当下游服务故障率超过阈值时，自动切断请求，快速失败并返回降级响应的保护机制。

**解决的问题**：下游服务故障导致调用方资源耗尽、雪崩效应。

**项目应用**：AI服务调用超时率超过50%时触发熔断，前端降级为"生成中，稍后查看"，避免大量请求堆积拖垮系统。

### 5.12 限流（Rate Limiting）

**定义**：限制单位时间内请求数量，防止过载的流量控制手段。

**解决的问题**：突发流量、恶意攻击导致系统崩溃。

**项目应用**：Gateway层限制单IP每秒100次请求，单用户每分钟调用AI接口10次，超限返回429，保护后端服务。

### 5.13 重试（Retry）

**定义**：请求失败后自动重新发起请求的策略，通常配合指数退避。

**解决的问题**：网络抖动、服务短暂不可用导致请求失败。

**项目应用**：gRPC调用失败自动重试3次，间隔100ms→200ms→400ms（指数退避），提升成功率到99.9%。

## 六、安全与认证机制

### 6.1 OAuth2

**定义**：开放授权标准，允许第三方应用在不获取用户密码的情况下访问用户资源。

**解决的问题**：用户不愿直接提供密码给第三方、密码传播风险高。

**项目应用**：支持微信、Google等第三方登录，用户授权后系统获得Access Token访问用户基本信息，无需存储密码。

### 6.2 OIDC（OpenID Connect）

**定义**：基于OAuth2的身份认证协议,提供标准化的用户身份信息（ID Token）。

**解决的问题**：OAuth2只解决授权，不解决身份认证。

**项目应用**：用户登录后签发JWT格式的ID Token，包含用户ID、昵称、头像等标准字段，多站点通过统一的OIDC Provider实现单点登录（SSO）。

### 6.3 RBAC（Role-Based Access Control，基于角色的访问控制）

**定义**：给用户分配角色（如管理员、编辑、查看者），角色关联权限，用户通过角色获得权限的模型。

**解决的问题**：直接给用户分配权限导致管理复杂、权限爆炸。

**项目应用**：定义"免费用户、VIP用户、企业管理员"等角色，不同角色拥有不同的导出次数、AI调用额度、模板访问权限。

### 6.4 ABAC（Attribute-Based Access Control，基于属性的访问控制）

**定义**：根据用户属性、资源属性、环境属性（时间、地点）等动态计算权限的细粒度模型。

**解决的问题**：RBAC无法满足"只能访问自己创建的资源"等复杂场景。

**项目应用**：用户删除项目时,校验"用户ID == 项目创建者ID 且 项目状态 != 已归档"等条件，实现细粒度权限控制。

### 6.5 JWT（JSON Web Token）

**定义**：一种轻量级的令牌格式，包含头部、载荷、签名三部分，可自包含用户信息和权限。

**解决的问题**：传统Session需要服务端存储、不利于分布式系统扩展。

**项目应用**：用户登录后签发JWT，前端存储在LocalStorage，每次请求携带在Authorization头，Gateway验签后提取用户信息，无需查库，支持无状态扩展。

## 七、性能与可用性指标

### 7.1 TTFB（Time to First Byte，首字节时间）

**定义**：从浏览器发起请求到接收到服务器第一个字节响应的时间。

**意义**：衡量服务器响应速度，影响用户感知的"等待时间"。

**项目目标**：约200ms，通过CDN、缓存、SSR优化实现。

### 7.2 首屏时间（First Contentful Paint, FCP）

**定义**：从页面开始加载到首次渲染任何文本、图像的时间。

**意义**：衡量页面"可见速度"，直接影响用户第一印象和跳出率。

**项目目标**：约1s，通过SSR、代码分割、资源预加载优化。

### 7.3 P95/P99延迟

**定义**：将所有请求按响应时间排序，第95/99百分位的值，即95%/99%的请求响应时间在此之内。

**意义**：比平均值更能反映用户真实体验（过滤极端慢请求的影响）。

**项目目标**：接口P95=150ms、P99=300ms，通过缓存、索引优化、异步化实现。

### 7.4 QPS（Queries Per Second，每秒查询数）

**定义**：系统每秒能处理的请求数量。

**意义**：衡量系统吞吐能力。

**项目成果**：通过异步化、缓存、扩容，峰值QPS提升2-3倍，达到数万级。

### 7.5 SLA（Service Level Agreement，服务等级协议）

**定义**：服务提供者承诺的可用性目标，如"99.95%可用"。

**意义**：量化可靠性承诺，99.95%意味着每年停机时间不超过4.38小时。

**项目目标**：SLA ≥ 99.95%，通过多副本、熔断降级、快速回滚保障。

### 7.6 MTTR（Mean Time to Repair/Recovery，平均修复时间）

**定义**：从故障发生到恢复正常的平均时间。

**意义**：衡量应急响应能力，MTTR越短，用户受影响时间越短。

**项目成果**：通过蓝绿部署、一键回滚，MTTR降至2-5分钟。

### 7.7 SLO（Service Level Objective，服务等级目标）

**定义**：比SLA更细化的内部目标，如"P99延迟<300ms"、"错误率<0.1%"。

**意义**：指导日常优化方向，避免过度工程或欠优化。

**项目实践**：为每个核心接口设定SLO，通过监控告警持续跟踪，发现劣化及时优化。

### 7.8 导出成功率

**定义**：导出任务完成数/发起数的百分比。

**意义**：直接影响用户体验和转化，是核心业务指标。

**项目成果**：通过异步队列、重试、幂等设计，成功率从95%提升至99.5%以上。

### 7.9 推理成本

**定义**：AI模型推理（生成图像、文本）的单次成本，包括算力、调用API费用。

**意义**：直接影响项目盈利能力。

**项目成果**：通过模型选择（小模型优先）、缓存结果、批量推理，推理成本降低30%-50%。

### 7.10 人效

**定义**：人均产出，如人均完成功能数、人均支撑的用户量。

**意义**：衡量工程效率和组织能力。

**项目成果**：通过Monorepo、组件复用、CI/CD自动化,人效显著提升，团队规模不变情况下支撑三站迭代。
