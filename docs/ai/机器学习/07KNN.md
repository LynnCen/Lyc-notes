一、KNN 
1. 例题:运用KNN进行影片分类 

核心思想：物以类聚，人以群分，通过样本周边k个最近邻的类别来判断样本属性
特征选择：使用搞笑镜头、拥抱镜头、打斗镜头三个特征维度
标签编码：
0：喜剧片
1：动作片
2：爱情片
预测目标：判断[23,3,17]（唐人街探案）的电影类型
2. KNN模型API介绍 
1）导入线性回归包 

依赖包：from sklearn.neighbors import KNeighborsClassifier
注意事项：导入时注意大小写规范，IDE可能不会自动提示小写开头的包名
2）准备数据 

特征数据：二维列表结构，每个样本包含三个特征值
标签数据：一维列表，对应每个样本的类别编码
3）实例化线性回归模型 

关键参数：n_neighbors指定考虑的最近邻数量
参数选择：k值过小容易过拟合，过大可能欠拟合，需根据数据特点调整
4）模型预测 

训练方法：estimator.fit(x, y) 学习特征与标签的映射关系
预测语法：predict()方法接收待预测样本的特征数组
5）应用案例 
例题:KNN模型编程实践

实现步骤：
导入KNeighborsClassifier
准备特征数据x和标签y
实例化模型（设置n_neighbors=3）
调用fit方法训练模型
使用predict进行预测
预测结果：唐人街探案被分类为0（喜剧片）
注意事项：输入特征必须保持与训练数据相同的维度顺序（搞笑、拥抱、打斗镜头）
二、代码实践 
1. 实例化模型训练模型预测 

实现步骤：
导入依赖包：from sklearn.neighbors import KNeighborsClassifier
准备数据：构建特征矩阵x和标签向量y
实例化模型：knn_cls = KNeighborsClassifier(n_neighbors=3)
模型训练：knn_cls.fit(x, y)
模型预测：knn_cls.predict([[23,3,17]])
数据格式：
特征矩阵示例：
标签向量示例：
类别编码：0-喜剧片，1-动作片，2-爱情片
2. 应用案例 
1）例题：KNN文档编程实践

数据准备：
包含9部电影的特征数据
特征维度：搞笑镜头、拥抱镜头、打斗镜头
示例数据：
功夫熊猫：[39,0,31]→喜剧片(0)
叶问：[3,2,65]→动作片(1)
二次曝光：[2,3,55]→爱情片(2)
模型参数：
n_neighbors=3：使用3个最近邻进行投票
默认使用均匀权重weights="uniform"
距离度量使用闵可夫斯基距离metric="minkowski"

关键实现：
预测方法区别：
predict()：直接返回预测类别
predict_proba()：返回各类别概率
命名规范：避免使用knn.py等与库重名的文件名
代码格式化：使用适当空行分隔不同功能模块
预测结果分析：
测试数据[23,3,17]预测结果为0（喜剧片）
模型通过计算3个最近邻的类别进行投票决策
可通过predict_proba()查看各类别概率分布

注意事项：
数据预处理：确保特征尺度一致
参数调优：可通过交叉验证选择最佳K值
模型评估：需要划分训练集和测试集
多分类支持：KNN天然支持多分类任务
三、知识小结
知识点
核心内容
易混淆点/关键细节
应用示例
KNN算法原理
基于“物以类聚，人以群分”思想，通过计算样本与周围K个邻居的相似性进行分类
- K值选择：需手动设定（如K=3）
- 距离度量：默认欧式距离，但未明确说明
电影类型分类：通过“搞笑镜头”“动作镜头”等特征判断喜剧/动作/爱情片
算法实现步骤
1. 导入sklearn.neighbors的KNeighborsClassifier
2. 准备特征数据（二维列表）和标签
3. 实例化模型并训练（.fit()）
4. 预测新样本（.predict()）
- 数据格式：特征需为二维数组
- 预测输出：.predict_proba()返回概率，而非直接类别
代码示例：
python<br>knn = KNeighborsClassifier(n_neighbors=3)<br>knn.fit(X_train, y_train)<br>pred = knn.predict([[20, 23, 17]])<br>
应用场景
适用于小规模、低维数据的分类任务（如电影推荐、用户画像）
- 局限性：高维数据性能下降（“维度灾难”）
- 特征重要性：需人工定义（如镜头类型权重）
案例：通过“厨房物品”推断用户爱好（读书/烹饪）
参数调优
n_neighbors（K值）影响模型复杂度：
- 小K值：过拟合风险
- 大K值：欠拟合风险
- 默认值：n_neighbors=5
- 动态调整：需交叉验证选择最优K
实验对比：K=3时分类准确率 vs K=5时泛化能力
与其他算法对比
- vs 决策树：KNN无显式训练过程，依赖存储全部数据
- vs SVM：KNN计算成本随数据量线性增长
共同点：均可处理多分类问题
实际选择：数据量小时用KNN，特征复杂时用决策树
