# K 近邻算法（KNN）与距离度量

> **核心思想**：物以类聚，人以群分 —— 通过样本周边 \(k\) 个最近邻的类别进行投票，从而判断样本属性。

---

## 一、KNN 算法入门

### 1. 例题：运用 KNN 进行影片分类

| 项目 | 说明 |
|------|------|
| **特征选择** | 搞笑镜头、拥抱镜头、打斗镜头（三个特征维度） |
| **标签编码** | 0：喜剧片 \| 1：动作片 \| 2：爱情片 |
| **预测目标** | 判断 `[23, 3, 17]`（《唐人街探案》）的电影类型 |

算法根据该样本在特征空间中与训练集中各样本的距离，找出最近的 \(k\) 个邻居，再按多数投票得到预测类别。

---

### 2. KNN 模型 API 介绍

#### （1）导入分类器

```python
from sklearn.neighbors import KNeighborsClassifier
```

- 注意包名大小写：`KNeighborsClassifier` 中 K、N、C 为大写，IDE 可能不会自动补全小写开头的名称。

#### （2）准备数据

- **特征数据**：二维数组/列表，每行一个样本，每列一个特征（如：搞笑、拥抱、打斗镜头数）。
- **标签数据**：一维数组，与样本一一对应，取值为类别编码（0/1/2）。

#### （3）实例化 KNN 模型

```python
knn_cls = KNeighborsClassifier(n_neighbors=3)
```

- **`n_neighbors`**：参与投票的最近邻数量 \(k\)。
- \(k\) 过小：容易过拟合，对噪声敏感；\(k\) 过大：容易欠拟合，边界模糊。需结合数据用交叉验证等方式调优。

#### （4）训练与预测

- **训练**：`estimator.fit(X, y)`，学习“特征 → 标签”的映射（KNN 实质是记住训练集，无显式模型参数）。
- **预测**：`predict(X_new)` 接收待预测样本的特征数组（保持与训练时相同的维度和顺序）。

#### （5）应用小结

- 流程：导入 `KNeighborsClassifier` → 准备 `x`、`y` → 实例化（如 `n_neighbors=3`）→ `fit(x, y)` → `predict(...)`。
- 本例中《唐人街探案》`[23, 3, 17]` 被预测为 **0（喜剧片）**。
- 输入特征顺序须与训练一致：搞笑、拥抱、打斗镜头。

---

## 二、代码实践

### 1. 基本流程：实例化 → 训练 → 预测

```python
from sklearn.neighbors import KNeighborsClassifier

# 特征矩阵（每行一部电影：搞笑、拥抱、打斗镜头）
x = [
    [39, 0, 31],   # 功夫熊猫
    [3, 2, 65],    # 叶问
    [2, 3, 55],    # 二次曝光
    # ... 共 9 部电影
]
# 标签向量：0-喜剧片，1-动作片，2-爱情片
y = [0, 1, 2, ...]

# 实例化并训练
knn_cls = KNeighborsClassifier(n_neighbors=3)
knn_cls.fit(x, y)

# 预测《唐人街探案》
result = knn_cls.predict([[23, 3, 17]])  # 输出: [0] → 喜剧片
```

---

### 2. 应用案例：KNN 电影分类编程实践

#### 数据说明

- 9 部电影，每部 3 个特征：**搞笑镜头、拥抱镜头、打斗镜头**。
- 示例：
  - 《功夫熊猫》`[39, 0, 31]` → 喜剧片(0)
  - 《叶问》`[3, 2, 65]` → 动作片(1)
  - 《二次曝光》`[2, 3, 55]` → 爱情片(2)

#### 常用参数

| 参数 | 含义 | 本例 |
|------|------|------|
| `n_neighbors` | 最近邻个数 \(k\) | 3 |
| `weights` | 邻居权重 | 默认 `"uniform"`（等权投票） |
| `metric` | 距离度量 | 默认 `"minkowski"`（闵可夫斯基） |

#### 预测接口

- **`predict()`**：返回预测类别（如 0、1、2）。
- **`predict_proba()`**：返回属于各类别的概率，便于分析置信度。

#### 结果与注意点

- 测试样本 `[23, 3, 17]` 预测为 **0（喜剧片）**，由 3 个最近邻投票决定。
- 建议：
  - 特征做尺度统一（如标准化），避免某一维主导距离。
  - 用交叉验证选取合适的 \(k\)。
  - 划分训练集/测试集评估泛化能力。
- KNN 天然支持多分类，无需 One-vs-Rest 等额外策略。
- 文件名避免与库冲突（如不用 `knn.py`），并保持代码分段清晰。

---

## 三、知识小结

| 知识点 | 核心内容 | 易混淆点 / 关键细节 | 应用示例 |
|--------|----------|----------------------|----------|
| **KNN 原理** | 基于“近朱者赤”，用样本周围 \(k\) 个邻居的类别投票分类 | \(k\) 需人工设定（如 \(k=3\)）；默认距离多为欧氏（由 `metric` 决定） | 用搞笑/动作/爱情镜头数区分喜剧、动作、爱情片 |
| **实现步骤** | ① 导入 `KNeighborsClassifier` ② 准备二维特征 + 一维标签 ③ 实例化、`fit` ④ `predict` 预测 | 特征必须是二维数组；`predict_proba()` 返回概率而非类别编码 | `knn = KNeighborsClassifier(n_neighbors=3)` → `fit` → `predict([[20,23,17]])` |
| **应用场景** | 小规模、低维分类（电影类型、用户画像等） | 高维易受“维度灾难”影响；特征与权重需人工设计 | 用“厨房物品”等特征推断读书/烹饪等爱好 |
| **参数调优** | `n_neighbors` 控制模型复杂度 | 小 \(k\) 易过拟合，大 \(k\) 易欠拟合；默认 5；建议交叉验证选 \(k\) | 比较 \(k=3\) 与 \(k=5\) 的准确率与泛化 |
| **与其他算法** | 无显式训练，依赖存储全部数据；计算量随样本数增加 | 与决策树：KNN 无树结构；与 SVM：KNN 计算成本随数据线性增长；均可做多分类 | 数据量小可用 KNN；特征复杂、需可解释性时可考虑决策树 |

---

# 距离度量

## 一、学习目标与四种距离

- **掌握**：欧氏距离、曼哈顿距离的计算与含义。
- **了解**：切比雪夫距离、闵可夫斯基距离的定义与在 KNN 中的作用。
- 四种距离均以外文人名命名：Euclidean（欧几里得）、Manhattan（曼哈顿）、Chebyshev（切比雪夫）、Minkowski（闵可夫斯基）。

---

## 二、欧氏距离（Euclidean Distance）

- **别称**：欧几里得距离。
- **公式**（\(n\) 维空间）：

\[
d_{\text{Euclidean}}(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
\]

- **几何意义**：两点之间的直线距离。
- **示例**：
  - 二维：\((1,1)\) 与 \((2,2)\) → \(\sqrt{(2-1)^2 + (2-1)^2} = \sqrt{2}\)。
  - 三维：在二维基础上加上 \(z\) 坐标差的平方再开根。
- 是最直观的空间距离，sklearn 中 `metric="minkowski", p=2` 即欧氏距离。

---

## 三、曼哈顿距离（Manhattan Distance）

- **别称**：城市街区距离（City Block Distance）。
- **公式**：

\[
d_{\text{Manhattan}}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n}|x_i - y_i|
\]

- **几何意义**：只能沿坐标轴行走时的路径长度（横平竖直），如城市网格道路。
- **示例**：\((1,1)\) 到 \((2,2)\) → \(|2-1| + |2-1| = 2\)。
- **与欧氏关系**：同一对点上，曼哈顿距离 ≥ 欧氏距离；在直角路径下，红蓝两条直角路径等长。

---

## 四、切比雪夫距离（Chebyshev Distance）

- **典型应用**：国际象棋中国王的移动步数（可直行、横行、斜行，一步到相邻 8 格之一）。
- **公式**：取各维坐标差绝对值的最大值：

\[
d_{\text{Chebyshev}}(\mathbf{x}, \mathbf{y}) = \max_i |x_i - y_i|
\]

- **示例**：\((0,0)\) 到 \((3,4)\) → \(\max(3, 4) = 4\) 步。

---

## 五、闵可夫斯基距离（Minkowski Distance）

- **作用**：用同一公式统一表示上述多种距离。
- **公式**：

\[
d_{\text{Minkowski}}(\mathbf{x}, \mathbf{y}; p) = \left( \sum_{i=1}^{n}|x_i - y_i|^p \right)^{1/p}
\]

- **参数与对应关系**：

| \(p\) | 对应距离 |
|-------|----------|
| \(p=1\) | 曼哈顿距离 |
| \(p=2\) | 欧氏距离 |
| \(p \to \infty\) | 切比雪夫距离（极限形式为 \(\max_i |x_i - y_i|\)） |

sklearn 中 KNN 默认 `metric="minkowski"`，常配合 `p=2` 使用。

---

## 六、闵可夫斯基例题

**题目**：下列哪个是闵可夫斯基距离的通用表达式？

- A：\(p=1\) 时的形式（曼哈顿）
- B：\(p=2\) 时的形式（欧氏）
- C：\(p\to\infty\) 时的形式（切比雪夫）
- D：非标准距离表达式

**答案**：选 **D**。题目问的是“通用表达式”，A/B/C 都是特例，只有闵可夫斯基通式 \(d = \big(\sum |x_i - y_i|^p\big)^{1/p}\) 才是通用；若 D 描述的是该通式，则选 D，否则根据选项表述判断。考点在于区分“通式”与“特例”。

---

## 七、距离度量小结表

| 知识点 | 核心内容 | 考试/易错点 | 难度 |
|--------|----------|-------------|------|
| **欧氏距离** | 直线距离，差方和开根 \(\sqrt{\sum(x_i-y_i)^2}\) | 常写为“欧几里得距离”；适用于任意 \(n\) 维 | ⭐⭐ |
| **曼哈顿距离** | 城市街区距离，绝对误差之和 \(\sum|x_i-y_i|\) | 联想网格道路；与欧氏几何意义不同 | ⭐⭐ |
| **切比雪夫距离** | 棋盘王移动，取绝对误差最大值 \(\max_i|x_i-y_i|\) | 场景：棋盘路径；与曼哈顿“最大值”逻辑不同 | ⭐⭐⭐ |
| **闵可夫斯基距离** | 通式 \(\big(\sum|x_i-y_i|^p\big)^{1/p}\)，\(p=1/2/\infty\) 对应以上三种 | \(p=1\) 曼哈顿，\(p=2\) 欧氏，\(p=\infty\) 切比雪夫 | ⭐⭐⭐⭐ |
| **对比与选择** | 欧氏：平方和开根；曼哈顿：绝对和；切比雪夫：最大值 | 题目常给四个坐标点，要求选对公式或算距离 | ⭐⭐⭐ |

---

*文档已按 KNN 流程与距离度量两条线整理，并补充公式与表格，便于复习与查阅。*
