# 其他机器学习方法

## 梯度下降

是什么？

梯度下降（Gradient Descent）是一类用于最小化目标函数（损失函数）的数值优化方法。它通过计算目标函数对参数的梯度，沿着梯度的反方向迭代更新参数，使损失逐步下降，直到收敛到局部极小或鞍点附近。

基本更新规则：\(\theta := \theta - \eta \, \nabla_\theta J(\theta)\)（\(\eta\) 为学习率，\(J\) 为损失函数）。

有什么用：

- 通用优化器：广泛用于线性/逻辑回归、神经网络等模型训练。
- 可扩展：支持海量数据与高维参数，结合小批量训练在GPU上高效运行。
- 可组合：容易与动量、正则化、学习率调度等技巧结合。

### 变体

- 批量梯度下降（BGD）：每次用全部训练数据计算一次梯度。
  - 优点：梯度稳定，单步指向更准确。
  - 缺点：大数据集成本高，内存占用大。
- 随机梯度下降（SGD）：每次用单个样本更新。
  - 优点：计算便宜、可跳出部分局部极小值。
  - 缺点：噪声大，收敛曲折，需要较小学习率和更多迭代。
- 小批量梯度下降（Mini-batch SGD）：每次用一个小批量（如32/64）样本更新。
  - 优点：折中方案，GPU 友好，是深度学习的默认选择。

### 学习率与收敛

- 学习率过大可能发散，过小训练缓慢。通常配合学习率调度（Step、Cosine、Warmup、ReduceLROnPlateau）与早停（Early Stopping）。
- 非凸问题常见“鞍点”，动量或自适应方法常能加速穿越。

### 常见改进优化器

- Momentum：一阶动量累计，抑制震荡，加速下降。
- Nesterov：在“预估位置”上计算梯度，收敛更稳。
- Adagrad：按参数历史梯度自适应调整学习率，对稀疏特征友好。
- RMSProp：指数加权历史二阶矩，缓解 Adagrad 学习率单调下降问题。
- Adam/AdamW：结合一阶与二阶矩估计（以及权重衰减拆解的 AdamW），是当前最常用的默认优化器。

### 代码示例

简易 Numpy 版（最小化 \(f(\mathbf{w})=\|\mathbf{w}\|_2^2\)）：

```python
import numpy as np

def loss(w):
    return (w ** 2).sum()

def grad(w):
    return 2 * w

w = np.array([3.0, -4.0])
lr = 0.1
for t in range(20):
    w -= lr * grad(w)
print("w=", w, "loss=", loss(w))
```

PyTorch 使用 Adam：

```python
import torch

w = torch.randn(10, requires_grad=True)
opt = torch.optim.Adam([w], lr=1e-2)
for _ in range(100):
    loss = (w ** 2).sum()
    opt.zero_grad()
    loss.backward()
    opt.step()
```

## 模型中的参数与超参数

是什么？

- 参数（Parameters）：模型通过训练从数据中“学得”的量，如线性模型的权重与偏置、神经网络的各层权重。
- 超参数（Hyperparameters）：在训练前需要设定、用于控制训练过程与模型容量的配置，如学习率、批量大小、正则化强度、树的深度、网络层数等。

有什么用：

- 参数 决定了模型的具体映射关系，是最终部署时实际使用的数值。
- 超参数 决定了模型的容量与训练动态，影响偏差-方差权衡、收敛速度与最终泛化性能。

### 常见超参数举例

- 优化相关：学习率、动量、权重衰减（L2）、学习率调度策略、训练轮数。
- 训练组织：批量大小、梯度裁剪阈值、早停耐心值。
- 结构容量：网络层数/宽度、激活函数、Dropout 比例、卷积核大小。
- 传统模型：
  - SVM：C、gamma、核函数类型。
  - kNN：n_neighbors、距离度量。
  - 决策树/随机森林：max_depth、n_estimators、min_samples_split。
  - 梯度提升/XGBoost：learning_rate、max_depth、subsample、colsample_bytree。

### 小示例（参数 vs 超参数）

```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(penalty="l2", C=1.0, max_iter=1000)  # C 是超参数
clf.fit(X_train, y_train)
print("参数 w=", clf.coef_)  # 训练得到的参数
```

### 超参数优化

目标是在给定搜索空间与评价指标（如 accuracy、f1、auc）下，自动找到性能最优的一组超参数。常见方法：

- 网格搜索（Grid Search）：遍历离散网格，简单但成本高。
- 随机搜索（Random Search）：随机采样组合，往往比网格更高效。
- 贝叶斯优化（Bayesian Optimization）：构建代理模型（如高斯过程、TPE）来指导下一次试验，更省试验次数。
- 多臂老虎机/早停（Hyperband/ASHA）：对“差的配置”尽早停训，把资源留给更有潜力的配置。

结合交叉验证的 Scikit-learn 示例：

```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Grid Search
d = {"n_estimators": [100, 200], "max_depth": [None, 10, 20]}
grid = GridSearchCV(RandomForestClassifier(random_state=42), d, cv=cv, scoring="accuracy")
grid.fit(X_train, y_train)
print(grid.best_params_, grid.best_score_)

# Random Search
param_dist = {"n_estimators": randint(100, 400), "max_depth": [None, 10, 20, 30]}
rand = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_dist,
                          n_iter=20, cv=cv, scoring="f1", random_state=42)
rand.fit(X_train, y_train)
print(rand.best_params_, rand.best_score_)
```

进阶工具：Optuna、Ray Tune、Hyperopt、scikit-optimize 等。

## 交叉验证

是什么？

交叉验证（Cross-Validation, CV）是一种更稳健估计模型泛化能力的方法。它将训练数据划分为多折（folds），循环使用其中一折做验证、其余折做训练，最终以多次验证得分的均值与方差评估模型表现。

有什么用：

- 减少由于一次性训练/验证划分带来的偶然性，尤其适合样本较少的场景。
- 为模型选择与超参数搜索提供可靠的评价机制。

### 常见方式

- K 折交叉验证（K-Fold）：最常见，n_splits=k。
- 分层 K 折（Stratified K-Fold）：分类任务保持各折类别分布一致。
- 留一法（LOOCV）：每次留一个样本验证，代价高但利用率最高。
- 时间序列分割（Time Series Split）：保持时间顺序，仅用过去预测未来，避免信息泄漏。
- 分组/组别交叉验证（Group K-Fold）：确保同组样本不被同时分到训练与验证。

### 注意事项

- 所有的特征工程、标准化、特征选择都必须在“每折的训练子集上 fit，再在验证子集上 transform”，避免数据泄漏。
- 若存在独立测试集（Hold-out），交叉验证只在训练集内部进行；最终报告在测试集上的一次性评分。
- 时序任务优先使用 TimeSeriesSplit，不要随机打乱时间顺序。

### 代码示例

交叉验证打分：

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
clf = RandomForestClassifier(n_estimators=200, random_state=42)
scores = cross_val_score(clf, X, y, cv=cv, scoring="accuracy")
print(scores.mean(), scores.std())
```

时间序列分割：

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
for fold, (tr, va) in enumerate(tscv.split(X)):
    X_tr, X_va = X[tr], X[va]
    y_tr, y_va = y[tr], y[va]
    # 训练并评估...
```

### 嵌套交叉验证（Nested CV）

用于更“无偏”的泛化评估：外层 CV 用于评估；内层 CV 用于模型与超参数选择。这样可以避免将同一验证数据既用于选择又用于评估而造成的乐观偏差。

