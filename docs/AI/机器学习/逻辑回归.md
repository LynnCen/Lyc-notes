# 逻辑回归

## 概述

逻辑回归（Logistic Regression）是一种广泛应用于二分类问题的统计学习方法。尽管名字中包含"回归"，但它实际上是一种分类算法。

### 基本特点

- **本质**：分类问题（通常用于二分类：0 或 1）
- **模型输出**：原始输出是连续值（−∞ ~ +∞）
- **核心机制**：使用Sigmoid函数将连续输出压缩到[0,1]区间
- **概率解释**：压缩后的结果解释为"样本属于类别 1 的概率"

### 核心思想

逻辑回归的核心是：使用一个线性模型得到连续输出 z，再使用Sigmoid函数把 z 压缩到 0-1 之间。将结果解释为"样本属于类别1的概率"，最终用阈值（通常为0.5）实现分类。

### 为什么叫"回归"？

逻辑回归之所以被称为"回归"，是因为它使用了线性回归的思想来建立模型，只是通过Sigmoid函数将输出转换为概率值，从而用于分类任务。

## 数学原理

### Sigmoid函数

Sigmoid函数是逻辑回归的核心，它将任意实数映射到(0,1)区间：

$$\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}$$

**Sigmoid函数的特性：**

- **值域**：(0, 1)，输出可以解释为概率
- **单调性**：严格单调递增
- **对称性**：关于点(0, 0.5)中心对称
- **导数特性**：$\sigma'(z) = \sigma(z)(1 - \sigma(z))$，这个特性使得梯度计算非常方便

**Sigmoid函数图像特点：**

- 当 z → +∞ 时，σ(z) → 1
- 当 z → -∞ 时，σ(z) → 0
- 当 z = 0 时，σ(z) = 0.5

### 逻辑回归模型

对于二分类问题，逻辑回归模型的数学表达式为：

$$h_\theta(x) = \sigma(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}}$$

其中：

- $h_\theta(x)$ 表示样本 x 属于类别 1 的概率
- $\theta$ 是模型参数（权重向量）
- $x$ 是特征向量
- $\theta^T x = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n$ 是线性组合

### 决策规则

给定阈值 t（通常为 0.5），分类决策规则为：

- 如果 $h_\theta(x) \geq t$，预测为类别 1
- 如果 $h_\theta(x) < t$，预测为类别 0

## 决策边界

**定义**：决策边界是把分类问题中的"0 类"和"1 类"分隔开的那条线／面／曲面。

**数学表达**：决策边界由方程 $\theta^T x = 0$ 确定，即：

$$\theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n = 0$$

**特点：**

- 对于二维特征空间，决策边界是一条直线
- 对于三维特征空间，决策边界是一个平面
- 对于更高维空间，决策边界是一个超平面
- 决策边界是线性的，这是逻辑回归的一个重要特性

## 成本函数

### 为什么不能使用MSE？

**问题**：为什么逻辑回归不能使用线性回归的 MSE（均方误差）作为成本函数？

**答案**：使用MSE会出现两个严重问题：

1. **梯度下降不稳定**：MSE + Sigmoid函数的目标函数会变成非凸曲面，可能有多个局部最优，不像线性回归那样有唯一的最优解（一个凸函数）。这会导致梯度下降算法难以找到全局最优解。

2. **概率模型不合理**：分类本质是判断标签属于某个类的概率，MSE不能很好表达概率差异。例如，真实标签为1，预测概率为0.9和0.1时，MSE的惩罚可能不够明显。

### 交叉熵损失函数

逻辑回归使用**交叉熵损失（Cross-Entropy Loss）**作为成本函数，也称为对数损失（Log Loss）。

**单个样本的损失函数：**

$$L(y, h_\theta(x)) = -[y \log(h_\theta(x)) + (1-y) \log(1 - h_\theta(x))]$$

其中：

- $y$ 是真实标签（0 或 1）
- $h_\theta(x)$ 是预测概率

**损失函数的特性：**

- 当 $y = 1$ 时：$L = -\log(h_\theta(x))$，预测概率越接近1，损失越小
- 当 $y = 0$ 时：$L = -\log(1 - h_\theta(x))$，预测概率越接近0，损失越小
- 当预测完全错误时（$y=1$但$h_\theta(x) \to 0$），损失趋向于无穷大

**所有样本的平均损失（成本函数）：**

$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)}))]$$

其中 $m$ 是训练样本数量。

**为什么交叉熵损失更好？**

1. **凸函数特性**：交叉熵损失是凸函数，保证梯度下降能找到全局最优解
2. **概率解释**：与最大似然估计等价，符合概率模型的统计原理
3. **梯度特性**：梯度计算简单且稳定，有利于优化算法收敛

## 拟合与泛化

拟合：

模型对训练数据的适应程度。在逻辑回归中，拟合是指模型通过学习训练数据，找到能够区分不同类别的决策边界的过程。一个好的拟合应该能够捕捉到数据中的真实模式。

欠拟合：

模型过于简单，无法捕捉到数据中的基本模式。在逻辑回归中，欠拟合可能表现为：

- 特征不足或特征选择不当
- 模型复杂度太低（如线性决策边界无法处理非线性可分的数据）
- 训练不充分
- 表现：训练集和测试集上的准确率都很低

过拟合：

模型过度学习了训练数据的细节和噪声，导致对训练数据表现很好，但对新数据表现很差。在逻辑回归中，过拟合可能表现为：

- 模型过于复杂（如使用了过多特征或高阶特征）
- 训练数据量太少
- 决策边界过于复杂，完美地分割了训练数据但无法泛化
- 表现：训练集准确率很高，但测试集准确率明显下降

泛化：

模型对未见过的数据的预测能力。泛化能力是衡量模型好坏的核心指标。在逻辑回归中：

- 好的泛化意味着模型学到的决策边界能够适用于新的数据样本
- 泛化能力通过测试集或验证集的性能来评估
- 欠拟合和过拟合都会降低模型的泛化能力
- 理想状态是在训练集和测试集上都表现良好，说明模型既学到了数据中的真实模式，又不会过度记忆训练数据

## 梯度下降算法

### 梯度计算

为了最小化成本函数，需要计算梯度。对于逻辑回归，梯度计算公式为：

$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$

**向量形式：**

$$\nabla_\theta J(\theta) = \frac{1}{m} X^T (h_\theta(X) - y)$$

其中：

- $X$ 是特征矩阵（$m \times n$）
- $y$ 是标签向量（$m \times 1$）
- $h_\theta(X)$ 是预测概率向量

### 梯度下降更新规则

$$\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}$$

其中 $\alpha$ 是学习率（learning rate）。

**批量梯度下降（Batch Gradient Descent）：**

每次迭代使用所有训练样本计算梯度。

**随机梯度下降（Stochastic Gradient Descent）：**

每次迭代随机选择一个样本计算梯度，更新速度快但可能不稳定。

**小批量梯度下降（Mini-batch Gradient Descent）：**

每次迭代使用一小批样本（如32、64个）计算梯度，平衡了速度和稳定性。

### 学习率选择

- **学习率过大**：可能导致损失函数震荡，甚至发散
- **学习率过小**：收敛速度慢，训练时间长
- **自适应学习率**：可以使用Adam、RMSprop等优化算法自动调整学习率

## 正则化

正则化是防止过拟合的重要技术，通过在损失函数中添加惩罚项来限制模型复杂度。

### L2正则化（Ridge正则化）

在成本函数中添加L2范数惩罚项：

$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2$$

其中 $\lambda$ 是正则化参数。

**特点：**

- 使参数值趋向于0，但不会完全为0
- 对所有参数进行平滑的惩罚
- 有助于处理特征之间的多重共线性问题

### L1正则化（Lasso正则化）

在成本函数中添加L1范数惩罚项：

$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)}))] + \frac{\lambda}{m} \sum_{j=1}^{n} |\theta_j|$$

**特点：**

- 可以将某些参数压缩到0，实现特征选择
- 产生稀疏模型，有助于特征选择
- 对异常值更敏感

### 弹性网络（Elastic Net）

结合L1和L2正则化：

$$J(\theta) = \text{原始损失} + \lambda_1 \sum_{j=1}^{n} |\theta_j| + \lambda_2 \sum_{j=1}^{n} \theta_j^2$$

**正则化参数 $\lambda$ 的选择：**

- $\lambda$ 越大，正则化强度越大，模型越简单
- 通常通过交叉验证来选择最优的 $\lambda$ 值

## 多分类逻辑回归

### One-vs-Rest（OvR）

将多分类问题转化为多个二分类问题：

- 对于K个类别，训练K个二分类器
- 第k个分类器将第k类作为正类，其他所有类作为负类
- 预测时，选择概率最高的类别

### One-vs-One（OvO）

为每对类别训练一个二分类器：

- 对于K个类别，需要训练 $C_K^2 = \frac{K(K-1)}{2}$ 个分类器
- 预测时，采用投票机制选择得票最多的类别

### Softmax回归

Softmax回归是逻辑回归在多分类问题上的直接推广：

$$P(y=k|x) = \frac{e^{\theta_k^T x}}{\sum_{j=1}^{K} e^{\theta_j^T x}}$$

其中：

- $K$ 是类别数量
- $\theta_k$ 是第k类的参数向量
- 分母是归一化项，确保所有类别的概率和为1

**Softmax函数特性：**

- 输出是概率分布（所有值在[0,1]之间，且和为1）
- 当K=2时，Softmax回归等价于逻辑回归

## 模型评估指标

### 混淆矩阵

二分类问题的混淆矩阵包含四个指标：

|  | 预测为正类 | 预测为负类 |
|---|---|---|
| **实际为正类** | TP (真正例) | FN (假负例) |
| **实际为负类** | FP (假正例) | TN (真负例) |

### 常用评估指标

**准确率（Accuracy）：**

$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

所有预测正确的样本占总样本的比例。

**精确率（Precision）：**

$$\text{Precision} = \frac{TP}{TP + FP}$$

预测为正类的样本中，实际为正类的比例。

**召回率（Recall，灵敏度）：**

$$\text{Recall} = \frac{TP}{TP + FN}$$

实际为正类的样本中，被正确预测为正类的比例。

**F1分数：**

$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

精确率和召回率的调和平均数，平衡两者。

**特异性（Specificity）：**

$$\text{Specificity} = \frac{TN}{TN + FP}$$

实际为负类的样本中，被正确预测为负类的比例。

### ROC曲线和AUC

**ROC曲线（Receiver Operating Characteristic）：**

- 横轴：假正例率（FPR）= $\frac{FP}{FP + TN}$
- 纵轴：真正例率（TPR）= $\frac{TP}{TP + FN}$ = Recall
- 通过改变分类阈值，得到不同的(FPR, TPR)点，连接成曲线

**AUC（Area Under Curve）：**

- ROC曲线下的面积
- 取值范围：[0, 1]
- AUC = 1：完美分类器
- AUC = 0.5：随机分类器
- AUC越大，模型性能越好

### 如何选择评估指标？

- **类别平衡**：使用准确率
- **关注正类**：使用精确率和召回率
- **类别不平衡**：使用F1分数、AUC
- **医疗诊断**：通常更关注召回率（不能漏诊）
- **垃圾邮件检测**：通常更关注精确率（不能误判正常邮件）

## 优缺点

### 优点

1. **实现简单**：算法原理清晰，易于理解和实现
2. **计算高效**：训练和预测速度快，适合大规模数据
3. **可解释性强**：可以解释每个特征对结果的影响（通过系数）
4. **不需要特征缩放**：虽然缩放可能有助于优化，但不是必需的
5. **概率输出**：直接输出概率值，便于后续决策
6. **不易过拟合**：特别是使用正则化后
7. **在线学习**：可以增量更新模型

### 缺点

1. **线性决策边界**：只能处理线性可分的分类问题
2. **特征工程重要**：需要手动构造非线性特征（如多项式特征）
3. **对异常值敏感**：异常值可能显著影响模型
4. **需要大量样本**：在小样本情况下可能表现不佳
5. **假设特征独立**：虽然不严格要求，但特征间高度相关可能影响性能

## 应用场景

### 典型应用领域

1. **医疗诊断**：疾病预测、症状分析
2. **金融风控**：信用评分、欺诈检测
3. **市场营销**：客户流失预测、广告点击率预测
4. **自然语言处理**：文本分类、情感分析
5. **推荐系统**：用户行为预测
6. **图像识别**：作为基础分类器（虽然现在更多用深度学习）

### 适用条件

- **二分类或多分类问题**
- **数据量适中到较大**
- **特征与目标变量存在线性或近似线性关系**
- **需要模型可解释性**
- **对预测速度有要求**

## 与其他算法对比

### 逻辑回归 vs 线性回归

| 特性 | 逻辑回归 | 线性回归 |
|---|---|---|
| **任务类型** | 分类 | 回归 |
| **输出** | 概率值（0-1） | 连续值 |
| **激活函数** | Sigmoid | 无（或恒等函数） |
| **损失函数** | 交叉熵 | 均方误差 |
| **输出解释** | 类别概率 | 数值预测 |

### 逻辑回归 vs 支持向量机（SVM）

| 特性 | 逻辑回归 | SVM |
|---|---|---|
| **决策边界** | 线性 | 线性或非线性（核函数） |
| **输出** | 概率 | 类别 |
| **正则化** | L1/L2 | L2（软间隔） |
| **计算复杂度** | 低 | 中等 |
| **大数据集** | 适合 | 可能较慢 |

### 逻辑回归 vs 决策树

| 特性 | 逻辑回归 | 决策树 |
|---|---|---|
| **可解释性** | 系数解释 | 规则解释 |
| **特征关系** | 线性 | 非线性 |
| **过拟合风险** | 低（有正则化） | 高 |
| **特征重要性** | 系数大小 | 信息增益等 |

## 实践建议

### 数据预处理

1. **特征缩放**：虽然不必须，但标准化或归一化有助于梯度下降收敛
2. **处理缺失值**：填充、删除或使用特殊值
3. **特征选择**：使用L1正则化或特征重要性分析
4. **处理类别不平衡**：使用SMOTE、欠采样或调整类别权重

### 模型训练

1. **学习率选择**：从0.01开始，根据收敛情况调整
2. **迭代次数**：设置最大迭代次数，监控损失函数变化
3. **正则化参数**：使用交叉验证选择最优 $\lambda$
4. **阈值调整**：根据业务需求调整分类阈值（不一定是0.5）

### 模型优化

1. **特征工程**：构造多项式特征、交互特征
2. **集成方法**：结合多个逻辑回归模型
3. **超参数调优**：网格搜索或随机搜索
4. **交叉验证**：使用K折交叉验证评估模型

### 常见问题与解决方案

#### 问题1：模型不收敛

- 检查学习率是否过大
- 检查特征是否已标准化
- 检查数据是否有异常值

#### 问题2：过拟合

- 增加正则化强度
- 减少特征数量
- 增加训练数据量

#### 问题3：欠拟合

- 减少正则化强度
- 增加特征（包括多项式特征）
- 检查模型是否训练充分

#### 问题4：类别不平衡

- 使用类别权重
- 使用SMOTE等技术
- 调整分类阈值
